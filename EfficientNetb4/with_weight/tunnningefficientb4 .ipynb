{"cells":[{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2023-09-07T06:55:19.903546Z","iopub.status.busy":"2023-09-07T06:55:19.903152Z","iopub.status.idle":"2023-09-07T06:56:18.131686Z","shell.execute_reply":"2023-09-07T06:56:18.130418Z","shell.execute_reply.started":"2023-09-07T06:55:19.903512Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: tensorflow-ranking==0.5.0 in /opt/conda/lib/python3.10/site-packages (0.5.0)\n","Requirement already satisfied: absl-py>=0.1.6 in /opt/conda/lib/python3.10/site-packages (from tensorflow-ranking==0.5.0) (1.4.0)\n","Requirement already satisfied: numpy>=1.13.3 in /opt/conda/lib/python3.10/site-packages (from tensorflow-ranking==0.5.0) (1.23.5)\n","Requirement already satisfied: six>=1.10.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow-ranking==0.5.0) (1.16.0)\n","Requirement already satisfied: tensorflow-serving-api<3.0.0,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow-ranking==0.5.0) (2.12.1)\n","Requirement already satisfied: grpcio<2.0,>=1.24.3 in /opt/conda/lib/python3.10/site-packages (from tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow-ranking==0.5.0) (1.51.1)\n","Collecting protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 (from tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow-ranking==0.5.0)\n","  Downloading protobuf-4.24.2-cp37-abi3-manylinux2014_x86_64.whl (311 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m311.4/311.4 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n","\u001b[?25hCollecting tensorflow<3,>=2.12.0 (from tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow-ranking==0.5.0)\n","  Downloading tensorflow-2.13.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (524.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m524.1/524.1 MB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n","\u001b[?25hRequirement already satisfied: astunparse>=1.6.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow<3,>=2.12.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow-ranking==0.5.0) (1.6.3)\n","Collecting flatbuffers>=23.1.21 (from tensorflow<3,>=2.12.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow-ranking==0.5.0)\n","  Downloading flatbuffers-23.5.26-py2.py3-none-any.whl (26 kB)\n","Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow<3,>=2.12.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow-ranking==0.5.0) (0.4.0)\n","Requirement already satisfied: google-pasta>=0.1.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow<3,>=2.12.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow-ranking==0.5.0) (0.2.0)\n","Requirement already satisfied: h5py>=2.9.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow<3,>=2.12.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow-ranking==0.5.0) (3.9.0)\n","Collecting keras<2.14,>=2.13.1 (from tensorflow<3,>=2.12.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow-ranking==0.5.0)\n","  Downloading keras-2.13.1-py3-none-any.whl (1.7 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m60.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: libclang>=13.0.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow<3,>=2.12.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow-ranking==0.5.0) (16.0.0)\n","Requirement already satisfied: opt-einsum>=2.3.2 in /opt/conda/lib/python3.10/site-packages (from tensorflow<3,>=2.12.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow-ranking==0.5.0) (3.3.0)\n","Requirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from tensorflow<3,>=2.12.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow-ranking==0.5.0) (21.3)\n","Requirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from tensorflow<3,>=2.12.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow-ranking==0.5.0) (68.0.0)\n","Collecting tensorboard<2.14,>=2.13 (from tensorflow<3,>=2.12.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow-ranking==0.5.0)\n","  Downloading tensorboard-2.13.0-py3-none-any.whl (5.6 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m52.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n","\u001b[?25hCollecting tensorflow-estimator<2.14,>=2.13.0 (from tensorflow<3,>=2.12.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow-ranking==0.5.0)\n","  Downloading tensorflow_estimator-2.13.0-py2.py3-none-any.whl (440 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m440.8/440.8 kB\u001b[0m \u001b[31m41.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: termcolor>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow<3,>=2.12.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow-ranking==0.5.0) (2.3.0)\n","Collecting typing-extensions<4.6.0,>=3.6.6 (from tensorflow<3,>=2.12.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow-ranking==0.5.0)\n","  Downloading typing_extensions-4.5.0-py3-none-any.whl (27 kB)\n","Requirement already satisfied: wrapt>=1.11.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow<3,>=2.12.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow-ranking==0.5.0) (1.14.1)\n","Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow<3,>=2.12.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow-ranking==0.5.0) (0.32.0)\n","Requirement already satisfied: wheel<1.0,>=0.23.0 in /opt/conda/lib/python3.10/site-packages (from astunparse>=1.6.0->tensorflow<3,>=2.12.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow-ranking==0.5.0) (0.40.0)\n","Requirement already satisfied: google-auth<3,>=1.6.3 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.14,>=2.13->tensorflow<3,>=2.12.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow-ranking==0.5.0) (2.20.0)\n","Collecting google-auth-oauthlib<1.1,>=0.5 (from tensorboard<2.14,>=2.13->tensorflow<3,>=2.12.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow-ranking==0.5.0)\n","  Downloading google_auth_oauthlib-1.0.0-py2.py3-none-any.whl (18 kB)\n","Requirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.14,>=2.13->tensorflow<3,>=2.12.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow-ranking==0.5.0) (3.4.3)\n","Requirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.14,>=2.13->tensorflow<3,>=2.12.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow-ranking==0.5.0) (2.31.0)\n","Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard<2.14,>=2.13->tensorflow<3,>=2.12.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow-ranking==0.5.0)\n","  Downloading tensorboard_data_server-0.7.1-py3-none-manylinux2014_x86_64.whl (6.6 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m83.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n","\u001b[?25hRequirement already satisfied: werkzeug>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.14,>=2.13->tensorflow<3,>=2.12.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow-ranking==0.5.0) (2.3.7)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->tensorflow<3,>=2.12.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow-ranking==0.5.0) (3.0.9)\n","Requirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow<3,>=2.12.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow-ranking==0.5.0) (4.2.4)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow<3,>=2.12.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow-ranking==0.5.0) (0.2.7)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow<3,>=2.12.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow-ranking==0.5.0) (4.9)\n","Requirement already satisfied: urllib3<2.0 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow<3,>=2.12.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow-ranking==0.5.0) (1.26.15)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow<3,>=2.12.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow-ranking==0.5.0) (1.3.1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow<3,>=2.12.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow-ranking==0.5.0) (3.1.0)\n","Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow<3,>=2.12.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow-ranking==0.5.0) (3.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow<3,>=2.12.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow-ranking==0.5.0) (2023.7.22)\n","Requirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard<2.14,>=2.13->tensorflow<3,>=2.12.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow-ranking==0.5.0) (2.1.3)\n","Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow<3,>=2.12.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow-ranking==0.5.0) (0.4.8)\n","Requirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.10/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow<3,>=2.12.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow-ranking==0.5.0) (3.2.2)\n","Installing collected packages: flatbuffers, typing-extensions, tensorflow-estimator, tensorboard-data-server, protobuf, keras, google-auth-oauthlib, tensorboard, tensorflow\n","  Attempting uninstall: flatbuffers\n","    Found existing installation: flatbuffers 1.12\n","    Uninstalling flatbuffers-1.12:\n","      Successfully uninstalled flatbuffers-1.12\n","  Attempting uninstall: typing-extensions\n","    Found existing installation: typing_extensions 4.6.3\n","    Uninstalling typing_extensions-4.6.3:\n","      Successfully uninstalled typing_extensions-4.6.3\n","  Attempting uninstall: tensorflow-estimator\n","    Found existing installation: tensorflow-estimator 2.9.0\n","    Uninstalling tensorflow-estimator-2.9.0:\n","      Successfully uninstalled tensorflow-estimator-2.9.0\n","  Attempting uninstall: tensorboard-data-server\n","    Found existing installation: tensorboard-data-server 0.6.1\n","    Uninstalling tensorboard-data-server-0.6.1:\n","      Successfully uninstalled tensorboard-data-server-0.6.1\n","  Attempting uninstall: protobuf\n","    Found existing installation: protobuf 3.19.6\n","    Uninstalling protobuf-3.19.6:\n","      Successfully uninstalled protobuf-3.19.6\n","  Attempting uninstall: keras\n","    Found existing installation: keras 2.9.0\n","    Uninstalling keras-2.9.0:\n","      Successfully uninstalled keras-2.9.0\n","  Attempting uninstall: google-auth-oauthlib\n","    Found existing installation: google-auth-oauthlib 0.4.6\n","    Uninstalling google-auth-oauthlib-0.4.6:\n","      Successfully uninstalled google-auth-oauthlib-0.4.6\n","  Attempting uninstall: tensorboard\n","    Found existing installation: tensorboard 2.9.1\n","    Uninstalling tensorboard-2.9.1:\n","      Successfully uninstalled tensorboard-2.9.1\n","  Attempting uninstall: tensorflow\n","    Found existing installation: tensorflow 2.9.1\n","    Uninstalling tensorflow-2.9.1:\n","      Successfully uninstalled tensorflow-2.9.1\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","cudf 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\n","cuml 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\n","dask-cudf 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\n","apache-beam 2.46.0 requires dill<0.3.2,>=0.3.1.1, but you have dill 0.3.7 which is incompatible.\n","apache-beam 2.46.0 requires protobuf<4,>3.12.2, but you have protobuf 4.24.2 which is incompatible.\n","apache-beam 2.46.0 requires pyarrow<10.0.0,>=3.0.0, but you have pyarrow 11.0.0 which is incompatible.\n","chex 0.1.82 requires numpy>=1.25.0, but you have numpy 1.23.5 which is incompatible.\n","cudf 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.0.2 which is incompatible.\n","cuml 23.8.0 requires dask==2023.7.1, but you have dask 2023.8.1 which is incompatible.\n","dask-cudf 23.8.0 requires dask==2023.7.1, but you have dask 2023.8.1 which is incompatible.\n","dask-cudf 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.0.2 which is incompatible.\n","google-cloud-aiplatform 0.6.0a1 requires google-api-core[grpc]<2.0.0dev,>=1.22.2, but you have google-api-core 2.11.1 which is incompatible.\n","google-cloud-automl 1.0.1 requires google-api-core[grpc]<2.0.0dev,>=1.14.0, but you have google-api-core 2.11.1 which is incompatible.\n","google-cloud-bigquery 2.34.4 requires protobuf<4.0.0dev,>=3.12.0, but you have protobuf 4.24.2 which is incompatible.\n","google-cloud-bigtable 1.7.3 requires protobuf<4.0.0dev, but you have protobuf 4.24.2 which is incompatible.\n","google-cloud-pubsub 2.17.1 requires grpcio<2.0dev,>=1.51.3, but you have grpcio 1.51.1 which is incompatible.\n","google-cloud-vision 2.8.0 requires protobuf<4.0.0dev,>=3.19.0, but you have protobuf 4.24.2 which is incompatible.\n","kfp 2.0.1 requires google-cloud-storage<3,>=2.2.1, but you have google-cloud-storage 1.44.0 which is incompatible.\n","kfp 2.0.1 requires protobuf<4,>=3.13.0, but you have protobuf 4.24.2 which is incompatible.\n","kfp-pipeline-spec 0.2.2 requires protobuf<4,>=3.13.0, but you have protobuf 4.24.2 which is incompatible.\n","pydantic-core 2.6.3 requires typing-extensions!=4.7.0,>=4.6.0, but you have typing-extensions 4.5.0 which is incompatible.\n","pymc3 3.11.5 requires numpy<1.22.2,>=1.15.0, but you have numpy 1.23.5 which is incompatible.\n","pymc3 3.11.5 requires scipy<1.8.0,>=1.7.3, but you have scipy 1.11.2 which is incompatible.\n","tensorboardx 2.6 requires protobuf<4,>=3.8.0, but you have protobuf 4.24.2 which is incompatible.\n","tensorflow-decision-forests 1.4.0 requires tensorflow~=2.12.0, but you have tensorflow 2.13.0 which is incompatible.\n","tensorflow-metadata 0.14.0 requires protobuf<4,>=3.7, but you have protobuf 4.24.2 which is incompatible.\n","tensorflow-text 2.12.1 requires tensorflow<2.13,>=2.12.0; platform_machine != \"arm64\" or platform_system != \"Darwin\", but you have tensorflow 2.13.0 which is incompatible.\n","tensorflow-transform 0.14.0 requires protobuf<4,>=3.7, but you have protobuf 4.24.2 which is incompatible.\n","ydata-profiling 4.3.1 requires scipy<1.11,>=1.4.1, but you have scipy 1.11.2 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed flatbuffers-23.5.26 google-auth-oauthlib-1.0.0 keras-2.13.1 protobuf-4.21.12 tensorboard-2.13.0 tensorboard-data-server-0.7.1 tensorflow-2.13.0 tensorflow-estimator-2.13.0 typing-extensions-4.5.0\n"]}],"source":["!pip install tensorflow-ranking==0.5.0"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2023-09-07T06:56:18.136191Z","iopub.status.busy":"2023-09-07T06:56:18.134988Z","iopub.status.idle":"2023-09-07T06:57:04.956559Z","shell.execute_reply":"2023-09-07T06:57:04.955288Z","shell.execute_reply.started":"2023-09-07T06:56:18.136139Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting tensorflow==2.9.1\n","  Using cached tensorflow-2.9.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (511.7 MB)\n","Requirement already satisfied: absl-py>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.9.1) (1.4.0)\n","Requirement already satisfied: astunparse>=1.6.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.9.1) (1.6.3)\n","Collecting flatbuffers<2,>=1.12 (from tensorflow==2.9.1)\n","  Using cached flatbuffers-1.12-py2.py3-none-any.whl (15 kB)\n","Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.9.1) (0.4.0)\n","Requirement already satisfied: google-pasta>=0.1.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.9.1) (0.2.0)\n","Requirement already satisfied: grpcio<2.0,>=1.24.3 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.9.1) (1.51.1)\n","Requirement already satisfied: h5py>=2.9.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.9.1) (3.9.0)\n","Collecting keras<2.10.0,>=2.9.0rc0 (from tensorflow==2.9.1)\n","  Using cached keras-2.9.0-py2.py3-none-any.whl (1.6 MB)\n","Requirement already satisfied: keras-preprocessing>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.9.1) (1.1.2)\n","Requirement already satisfied: libclang>=13.0.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.9.1) (16.0.0)\n","Requirement already satisfied: numpy>=1.20 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.9.1) (1.23.5)\n","Requirement already satisfied: opt-einsum>=2.3.2 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.9.1) (3.3.0)\n","Requirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.9.1) (21.3)\n","Collecting protobuf<3.20,>=3.9.2 (from tensorflow==2.9.1)\n","  Using cached protobuf-3.19.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n","Requirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.9.1) (68.0.0)\n","Requirement already satisfied: six>=1.12.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.9.1) (1.16.0)\n","Collecting tensorboard<2.10,>=2.9 (from tensorflow==2.9.1)\n","  Using cached tensorboard-2.9.1-py3-none-any.whl (5.8 MB)\n","Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.9.1) (0.32.0)\n","Collecting tensorflow-estimator<2.10.0,>=2.9.0rc0 (from tensorflow==2.9.1)\n","  Using cached tensorflow_estimator-2.9.0-py2.py3-none-any.whl (438 kB)\n","Requirement already satisfied: termcolor>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.9.1) (2.3.0)\n","Requirement already satisfied: typing-extensions>=3.6.6 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.9.1) (4.5.0)\n","Requirement already satisfied: wrapt>=1.11.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.9.1) (1.14.1)\n","Requirement already satisfied: wheel<1.0,>=0.23.0 in /opt/conda/lib/python3.10/site-packages (from astunparse>=1.6.0->tensorflow==2.9.1) (0.40.0)\n","Requirement already satisfied: google-auth<3,>=1.6.3 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.10,>=2.9->tensorflow==2.9.1) (2.20.0)\n","Collecting google-auth-oauthlib<0.5,>=0.4.1 (from tensorboard<2.10,>=2.9->tensorflow==2.9.1)\n","  Using cached google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\n","Requirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.10,>=2.9->tensorflow==2.9.1) (3.4.3)\n","Requirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.10,>=2.9->tensorflow==2.9.1) (2.31.0)\n","Collecting tensorboard-data-server<0.7.0,>=0.6.0 (from tensorboard<2.10,>=2.9->tensorflow==2.9.1)\n","  Using cached tensorboard_data_server-0.6.1-py3-none-manylinux2010_x86_64.whl (4.9 MB)\n","Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.10,>=2.9->tensorflow==2.9.1) (1.8.1)\n","Requirement already satisfied: werkzeug>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.10,>=2.9->tensorflow==2.9.1) (2.3.7)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->tensorflow==2.9.1) (3.0.9)\n","Requirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow==2.9.1) (4.2.4)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow==2.9.1) (0.2.7)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow==2.9.1) (4.9)\n","Requirement already satisfied: urllib3<2.0 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow==2.9.1) (1.26.15)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow==2.9.1) (1.3.1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow==2.9.1) (3.1.0)\n","Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow==2.9.1) (3.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow==2.9.1) (2023.7.22)\n","Requirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard<2.10,>=2.9->tensorflow==2.9.1) (2.1.3)\n","Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow==2.9.1) (0.4.8)\n","Requirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.10/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow==2.9.1) (3.2.2)\n","Installing collected packages: keras, flatbuffers, tensorflow-estimator, tensorboard-data-server, protobuf, google-auth-oauthlib, tensorboard, tensorflow\n","  Attempting uninstall: keras\n","    Found existing installation: keras 2.13.1\n","    Uninstalling keras-2.13.1:\n","      Successfully uninstalled keras-2.13.1\n","  Attempting uninstall: flatbuffers\n","    Found existing installation: flatbuffers 23.5.26\n","    Uninstalling flatbuffers-23.5.26:\n","      Successfully uninstalled flatbuffers-23.5.26\n","  Attempting uninstall: tensorflow-estimator\n","    Found existing installation: tensorflow-estimator 2.13.0\n","    Uninstalling tensorflow-estimator-2.13.0:\n","      Successfully uninstalled tensorflow-estimator-2.13.0\n","  Attempting uninstall: tensorboard-data-server\n","    Found existing installation: tensorboard-data-server 0.7.1\n","    Uninstalling tensorboard-data-server-0.7.1:\n","      Successfully uninstalled tensorboard-data-server-0.7.1\n","  Attempting uninstall: protobuf\n","    Found existing installation: protobuf 4.21.12\n","    Uninstalling protobuf-4.21.12:\n","      Successfully uninstalled protobuf-4.21.12\n","  Attempting uninstall: google-auth-oauthlib\n","    Found existing installation: google-auth-oauthlib 1.0.0\n","    Uninstalling google-auth-oauthlib-1.0.0:\n","      Successfully uninstalled google-auth-oauthlib-1.0.0\n","  Attempting uninstall: tensorboard\n","    Found existing installation: tensorboard 2.13.0\n","    Uninstalling tensorboard-2.13.0:\n","      Successfully uninstalled tensorboard-2.13.0\n","  Attempting uninstall: tensorflow\n","    Found existing installation: tensorflow 2.13.0\n","    Uninstalling tensorflow-2.13.0:\n","      Successfully uninstalled tensorflow-2.13.0\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","cudf 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\n","cuml 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\n","dask-cudf 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\n","apache-beam 2.46.0 requires dill<0.3.2,>=0.3.1.1, but you have dill 0.3.7 which is incompatible.\n","apache-beam 2.46.0 requires pyarrow<10.0.0,>=3.0.0, but you have pyarrow 11.0.0 which is incompatible.\n","cudf 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.0.2 which is incompatible.\n","cudf 23.8.0 requires protobuf<5,>=4.21, but you have protobuf 3.19.6 which is incompatible.\n","cuml 23.8.0 requires dask==2023.7.1, but you have dask 2023.8.1 which is incompatible.\n","dask-cudf 23.8.0 requires dask==2023.7.1, but you have dask 2023.8.1 which is incompatible.\n","dask-cudf 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.0.2 which is incompatible.\n","google-cloud-aiplatform 0.6.0a1 requires google-api-core[grpc]<2.0.0dev,>=1.22.2, but you have google-api-core 2.11.1 which is incompatible.\n","google-cloud-automl 1.0.1 requires google-api-core[grpc]<2.0.0dev,>=1.14.0, but you have google-api-core 2.11.1 which is incompatible.\n","google-cloud-pubsub 2.17.1 requires grpcio<2.0dev,>=1.51.3, but you have grpcio 1.51.1 which is incompatible.\n","kfp 2.0.1 requires google-cloud-storage<3,>=2.2.1, but you have google-cloud-storage 1.44.0 which is incompatible.\n","onnx 1.14.1 requires protobuf>=3.20.2, but you have protobuf 3.19.6 which is incompatible.\n","tensorflow-datasets 4.9.2 requires protobuf>=3.20, but you have protobuf 3.19.6 which is incompatible.\n","tensorflow-decision-forests 1.4.0 requires tensorflow~=2.12.0, but you have tensorflow 2.9.1 which is incompatible.\n","tensorflow-serving-api 2.12.1 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3, but you have protobuf 3.19.6 which is incompatible.\n","tensorflow-serving-api 2.12.1 requires tensorflow<3,>=2.12.0, but you have tensorflow 2.9.1 which is incompatible.\n","tensorflow-text 2.12.1 requires tensorflow<2.13,>=2.12.0; platform_machine != \"arm64\" or platform_system != \"Darwin\", but you have tensorflow 2.9.1 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed flatbuffers-1.12 google-auth-oauthlib-0.4.6 keras-2.9.0 protobuf-3.19.6 tensorboard-2.9.1 tensorboard-data-server-0.6.1 tensorflow-2.9.1 tensorflow-estimator-2.9.0\n"]}],"source":["!pip install tensorflow==2.9.1"]},{"cell_type":"markdown","metadata":{},"source":["**Importing  libraries**"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2023-09-07T06:57:04.959341Z","iopub.status.busy":"2023-09-07T06:57:04.958877Z","iopub.status.idle":"2023-09-07T06:57:04.972885Z","shell.execute_reply":"2023-09-07T06:57:04.966302Z","shell.execute_reply.started":"2023-09-07T06:57:04.959299Z"},"trusted":true},"outputs":[],"source":["import os\n","import numpy as np\n","from tensorflow import keras\n","from tensorflow.keras.optimizers import Adam\n","from tensorflow.keras.applications import EfficientNetB4\n","from tensorflow_addons.metrics import F1Score\n","from tensorflow.keras.layers import Input, Dense, Dropout, GlobalAveragePooling2D\n","from tensorflow.keras import regularizers  \n","from kerastuner.tuners import RandomSearch\n","import tensorflow_ranking as tfr"]},{"cell_type":"markdown","metadata":{},"source":["### Custom Hamming Loss Metric\n","\n","In this code  a custom Hamming Loss metric is defined using TensorFlow and Keras. The Hamming Loss is a metric used to evaluate the accuracy of multi-label classification models. Here's an explanation of the code:\n","\n","#### Custom Metric Class\n","\n","- A custom metric class named `HammingLoss` is defined. This class extends the `Metric` class provided by TensorFlow/Keras.\n","\n","- The `__init__` method initializes the metric. It accepts parameters such as `threshold` and `name`. The threshold defines the threshold value for binary conversion of predictions. The `hamming_loss` and `count` variables are created as TensorFlow variables to keep track of the Hamming loss and the number of samples.\n","\n","#### `update_state` Method\n","\n","- The `update_state` method is used to update the state of the metric. It accepts `y_true` (true labels), `y_pred` (predicted probabilities), and `sample_weight` (optional).\n","\n","- The method first converts the predicted probabilities to binary labels based on the specified threshold.\n","\n","- It then computes the absolute differences between the true labels and the binary predictions for each sample and class.\n","\n","- The mean over classes for each sample is calculated, which represents the Hamming loss for that sample.\n","\n","- The Hamming loss and the sample count are updated accordingly.\n","\n","#### `result` Method\n","\n","- The `result` method calculates the final Hamming loss by dividing the accumulated Hamming loss by the sample count.\n","\n","#### `reset_state` Method\n","\n","- The `reset_state` method is used to reset the Hamming loss and sample count at the end of each epoch. This is important to ensure that the metric calculations are isolated for each epoch.\n","\n","This custom Hamming Loss metric can be used during the training of multi-label classification models to monitor and evaluate the model's performance with regard to label prediction accuracy.\n","\n","\n"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2023-09-07T06:57:04.977004Z","iopub.status.busy":"2023-09-07T06:57:04.976242Z","iopub.status.idle":"2023-09-07T06:57:05.028872Z","shell.execute_reply":"2023-09-07T06:57:05.027860Z","shell.execute_reply.started":"2023-09-07T06:57:04.976954Z"},"trusted":true},"outputs":[],"source":["import tensorflow as tf\n","from tensorflow.keras.metrics import Metric\n","\n","class HammingLoss(Metric):\n","    def __init__(self, threshold=0.5, name=\"hamming_loss\", **kwargs):\n","        super(HammingLoss, self).__init__(name=name, **kwargs)\n","        self.threshold = tf.Variable(threshold, trainable=False, dtype=tf.float32)\n","        self.hamming_loss = self.add_weight(name=\"hl\", initializer=\"zeros\")\n","        self.count = self.add_weight(name=\"count\", initializer=\"zeros\")\n","\n","    def update_state(self, y_true, y_pred, sample_weight=None):\n","        y_pred_binary = tf.cast(y_pred > self.threshold, tf.float32)\n","        y_true = tf.cast(y_true, tf.float32)\n","        tmp = tf.math.abs(y_true - y_pred_binary)\n","        hl = tf.math.reduce_mean(tmp, axis=-1)\n","        self.hamming_loss.assign_add(tf.math.reduce_sum(hl))\n","        self.count.assign_add(tf.cast(tf.size(y_true) / tf.shape(y_true)[-1], tf.float32))\n","    def result(self):\n","        return self.hamming_loss / self.count\n","    def reset_state(self):\n","        self.hamming_loss.assign(0.)\n","        self.count.assign(0.)"]},{"cell_type":"markdown","metadata":{},"source":["### Data Loading and Filtering\n","\n","This section covers the process of loading data from CSV files and applying filtering to include only specific rows based on the value of the `Disease_Risk` column.\n","\n","- The data is loaded from the following CSV files:\n","  - Training data: `train_file.csv`\n","  - Validation data: `val_file.csv`\n","  - Test data: `test_file.csv`\n","\n","- To ensure that the analysis focuses on samples relevant to disease risk, a filter is applied to select rows where the `Disease_Risk` column is equal to 1. This step helps exclude unrelated data.\n","\n","- Additionally, the disease labels are extracted into the `labels` variable for further reference.\n","\n","- Finally, information is provided on the sizes of the resulting datasets, including the number of samples in the training, validation, and test sets.\n","\n","This data preparation process ensures that subsequent analysis or model training is based on the most relevant samples with disease risk.\n"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2023-09-07T06:57:42.488310Z","iopub.status.busy":"2023-09-07T06:57:42.487139Z","iopub.status.idle":"2023-09-07T06:57:42.830971Z","shell.execute_reply":"2023-09-07T06:57:42.829878Z","shell.execute_reply.started":"2023-09-07T06:57:42.488262Z"},"trusted":true},"outputs":[{"data":{"text/plain":["(7502, 934, 953, 9389)"]},"execution_count":11,"metadata":{},"output_type":"execute_result"}],"source":["import pandas as pd\n","train_data = pd.read_csv('/kaggle/input/my-data/train_file.csv')\n","val_data = pd.read_csv('/kaggle/input/my-data/val_file.csv')\n","test_data = pd.read_csv('/kaggle/input/my-data/test_file.csv')\n","train_data = train_data[train_data['Disease_Risk'] == 1]\n","val_data = val_data[val_data['Disease_Risk'] == 1]\n","test_data = test_data[test_data['Disease_Risk'] == 1]\n","labels = train_data.columns[2:-1]\n","len_train_data = len(train_data)\n","len_val_data = len(val_data)\n","len_test_data = len(test_data)\n","filtered_size = len_train_data + len_val_data + len_test_data\n","\n","len(train_data), len(val_data), len(test_data),filtered_size"]},{"cell_type":"markdown","metadata":{},"source":["### Custom Image Augmentation Layer\n","\n","In the following code  a custom image augmentation layer is defined using TensorFlow and Keras. This custom layer is designed to apply various image augmentation operations for data preprocessing and augmentation in deep learning models. Here's an explanation of the code:\n","\n","#### Custom Image Augmentation Class\n","\n","- The code defines a class called `CustomImageAugmentation` that inherits from `tf.keras.layers.Layer`. This class serves as the foundation for applying image augmentation techniques to input images.\n","\n","- The class constructor (`__init__`) is responsible for initializing the image augmentation layer. It accepts a set of parameters that allow customization of which augmentation operations are applied. These operations include horizontal flipping, rotation, brightness adjustment, contrast adjustment, saturation adjustment, hue adjustment, scaling, cropping, grid distortion, compression, Gaussian noise, Gaussian blur, downscaling, gamma correction, and elastic transformation.\n","\n","- The `call` method within this class is used to apply the specified augmentation operations to input images. It includes a parameter named `apply` that controls whether the augmentations should be applied or not.\n","\n","- The augmentation operations are diverse and encompass random horizontal flipping, random rotation, random brightness, contrast, saturation, and hue adjustments, scaling, and more. The application of each operation is controlled by the corresponding class attribute, such as `self.flip` and `self.rotate`.\n","\n","- If the `apply` parameter is set to `True`, the augmentation operations are applied to the input images; otherwise, the original images are returned.\n","\n","- The `img_aug` variable represents an instance of the `CustomImageAugmentation` class, which can be used for data augmentation within image-based deep learning models.\n","\n","This custom image augmentation layer offers flexibility in specifying and applying image transformations, enhancing model generalization and performance when working with image datasets.\n"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2023-09-07T06:58:01.387021Z","iopub.status.busy":"2023-09-07T06:58:01.385888Z","iopub.status.idle":"2023-09-07T06:58:01.428871Z","shell.execute_reply":"2023-09-07T06:58:01.427700Z","shell.execute_reply.started":"2023-09-07T06:58:01.386980Z"},"trusted":true},"outputs":[],"source":["import tensorflow as tf\n","class CustomImageAugmentation(tf.keras.layers.Layer):\n","    def __init__(self, flip=True, rotate=True, brightness=True,\n","                 contrast=True, saturation=True, hue=True, scale=False,\n","                 crop=False, grid_distortion=False, compression=False,\n","                 gaussian_noise=False, gaussian_blur=False,\n","                 downscaling=False, gamma=False, elastic_transform=False, **kwargs):\n","        super(CustomImageAugmentation, self).__init__(**kwargs)\n","        self.flip = flip\n","        self.rotate = rotate\n","        self.brightness = brightness\n","        self.contrast = contrast\n","        self.saturation = saturation\n","        self.hue = hue\n","        self.scale = scale\n","        self.crop = crop\n","        self.grid_distortion = grid_distortion\n","        self.compression = compression\n","        self.gaussian_noise = gaussian_noise\n","        self.gaussian_blur = gaussian_blur\n","        self.downscaling = downscaling\n","        self.gamma = gamma\n","        self.elastic_transform = elastic_transform\n","\n","    def call(self, inputs, apply=True):\n","        if apply:\n","            augmented = tf.image.random_flip_left_right(inputs) if self.flip else inputs\n","            augmented = tf.image.rot90(augmented, k=tf.random.uniform(shape=[], minval=0, maxval=4, dtype=tf.int32)) if self.rotate else augmented\n","            augmented = tf.image.random_brightness(augmented, max_delta=0.2) if self.brightness else augmented\n","            augmented = tf.image.random_contrast(augmented, lower=0.5, upper=1.5) if self.contrast else augmented\n","            augmented = tf.image.random_saturation(augmented, lower=0.5, upper=1.5) if self.saturation else augmented\n","            augmented = tf.image.random_hue(augmented, max_delta=0.2) if self.hue else augmented\n","            return augmented\n","        else:\n","            return inputs\n","img_aug = CustomImageAugmentation()"]},{"cell_type":"markdown","metadata":{},"source":["### Custom Image Data Generator for Augmentation\n","\n","In this code a custom image data generator is defined using TensorFlow and Keras. This data generator is designed to apply various image augmentation operations to preprocess and augment data in deep learning models. Here's an explanation of the code:\n","\n","#### Custom Image Data Generator Class\n","\n","- A custom image data generator class is created using TensorFlow and Keras.\n","\n","- The data generator is configured with various augmentation operations that can be applied during data preprocessing.\n","\n","- The `preprocessing_function` parameter is defined as a lambda function that utilizes the `img_aug` function with the `apply=True` flag to apply augmentation operations.\n","\n","- The defined data generator can be used to preprocess and augment images during the training of deep learning models, enhancing the model's ability to learn from diverse and augmented data.\n","\n","\n"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2023-09-07T06:58:04.392895Z","iopub.status.busy":"2023-09-07T06:58:04.392545Z","iopub.status.idle":"2023-09-07T06:58:04.399769Z","shell.execute_reply":"2023-09-07T06:58:04.398755Z","shell.execute_reply.started":"2023-09-07T06:58:04.392866Z"},"trusted":true},"outputs":[],"source":["from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","\n","datagen = ImageDataGenerator(\n","    preprocessing_function=lambda x: img_aug(x, apply=True)\n",")"]},{"cell_type":"markdown","metadata":{},"source":["### Data Generator for Training\n","\n","In this code a data generator for training deep learning models is defined using TensorFlow and Keras. The data generator is configured to preprocess and augment the training data. Here's an explanation of the code:\n","\n","#### Data Generator Configuration\n","\n","- The data generator is configured with the specified `target_size` of (380, 380), which defines the desired size of input images.\n","\n","- The `batch_size` is set to 32, determining the number of samples to process in each batch during training.\n","\n","#### Data Flow Configuration\n","\n","- The `train_generator` is created using the `datagen.flow_from_dataframe` method.\n","\n","- It is associated with the training data stored in the `train_data` DataFrame.\n","\n","- The `x_col` parameter specifies the column name in the DataFrame where image file paths are stored.\n","\n","- The `y_col` parameter is set to the list of column names representing the labels in the DataFrame. These columns are obtained from `train_data.columns[2:-1].tolist()`.\n","\n","- The `class_mode` is set to 'raw', indicating that the generator should return raw arrays as the target values.\n","\n","- Images are processed in batches of size `batch_size`.\n","\n","- The `target_size` parameter is set to the specified dimensions of (380, 380) for image resizing.\n","\n","- Data shuffling is enabled with the `shuffle` parameter set to `True`.\n","\n","This data generator is essential for efficiently feeding training data to deep learning models, enabling data augmentation and resizing to match model input requirements.\n"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2023-09-07T06:58:05.966849Z","iopub.status.busy":"2023-09-07T06:58:05.966489Z","iopub.status.idle":"2023-09-07T06:58:28.075190Z","shell.execute_reply":"2023-09-07T06:58:28.074190Z","shell.execute_reply.started":"2023-09-07T06:58:05.966818Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Found 7502 validated image filenames.\n"]}],"source":["target_size=(380, 380)\n","batch_size=32\n","train_generator = datagen.flow_from_dataframe(\n","    dataframe=train_data,\n","    x_col='IMG_DIR',\n","    y_col=train_data.columns[2:-1].tolist(),\n","    class_mode='raw',\n","    batch_size=batch_size,\n","    target_size=target_size,\n","    shuffle=True\n",")"]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2023-09-07T06:58:28.077961Z","iopub.status.busy":"2023-09-07T06:58:28.077267Z","iopub.status.idle":"2023-09-07T06:58:30.785721Z","shell.execute_reply":"2023-09-07T06:58:30.784749Z","shell.execute_reply.started":"2023-09-07T06:58:28.077924Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Found 934 validated image filenames.\n"]}],"source":["val_generator = datagen.flow_from_dataframe(\n","    dataframe=val_data,\n","    x_col='IMG_DIR',\n","    y_col=val_data.columns[2:-1].tolist(),\n","    class_mode='raw',\n","    batch_size=batch_size,\n","    target_size=target_size,\n","    shuffle=False  \n",")"]},{"cell_type":"code","execution_count":16,"metadata":{"execution":{"iopub.execute_input":"2023-09-07T06:58:30.787502Z","iopub.status.busy":"2023-09-07T06:58:30.787159Z","iopub.status.idle":"2023-09-07T06:58:34.792522Z","shell.execute_reply":"2023-09-07T06:58:34.791524Z","shell.execute_reply.started":"2023-09-07T06:58:30.787468Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Batch images shape: (32, 380, 380, 3)\n","Batch labels shape: (32, 28)\n"]}],"source":["for batch_images, batch_labels in train_generator:\n","    print(\"Batch images shape:\", batch_images.shape)\n","    print(\"Batch labels shape:\", batch_labels.shape)\n","    break"]},{"cell_type":"markdown","metadata":{},"source":["### Custom Model Builder Function\n","\n","In this code  a custom model builder function is defined for creating a deep learning model using TensorFlow and Keras. This function is typically used with hyperparameter tuning techniques. Here's an explanation of the code:\n","\n","#### Model Building within a Strategy Scope\n","\n","- The `model_builder` function is designed to build a deep learning model within the context of a TensorFlow distribution strategy, specifically a `MirroredStrategy` for training on multiple GPUs.\n","\n","- Inside the strategy scope, the `EfficientNetB4` pre-trained model is used as the base model with pre-trained weights from 'imagenet'. The model is configured to exclude the top classification layers.\n","\n","- Input images are expected to have a specified shape, and the model's base layers are marked as not trainable by setting `base_model.trainable = False`.\n","\n","#### Hyperparameter Configuration\n","\n","- The function accepts hyperparameters using the `hp` object, which allows for tuning of various model aspects.\n","\n","- Hyperparameters include units and L2 regularization strengths for three fully connected dense layers.\n","\n","- The function also accepts a dropout rate within the range [0.3, 0.7] as another hyperparameter.\n","\n","#### Model Architecture\n","\n","- The model architecture consists of three dense layers, each followed by ReLU activation and L2 regularization. The number of units in these dense layers is determined by the hyperparameters.\n","\n","- A dropout layer follows the dense layers, and its rate is determined by a hyperparameter.\n","\n","- The final layer is a dense layer with a sigmoid activation, suitable for multi-label classification. The number of output units matches the number of labels.\n","\n","#### Model Compilation\n","\n","- The model is compiled with an Adam optimizer, a specified learning rate, and binary cross-entropy loss, which is common for multi-label classification tasks.\n","\n","- Multiple metrics are included in the compilation, such as accuracy, area under the curve (AUC), precision, recall, F1 score, Hamming loss, and mean average precision. These metrics help evaluate the model's performance on multi-label classification tasks.\n","\n","This custom model builder function is essential for creating deep learning models with the flexibility to tune hyperparameters and design custom architectures.\n","\n"]},{"cell_type":"code","execution_count":24,"metadata":{"execution":{"iopub.execute_input":"2023-09-07T07:00:39.139141Z","iopub.status.busy":"2023-09-07T07:00:39.138644Z","iopub.status.idle":"2023-09-07T07:00:39.171980Z","shell.execute_reply":"2023-09-07T07:00:39.170438Z","shell.execute_reply.started":"2023-09-07T07:00:39.139099Z"},"trusted":true},"outputs":[],"source":["def model_builder(hp):\n","    strategy = tf.distribute.MirroredStrategy()\n","    with strategy.scope():\n","        base_model = EfficientNetB4(weights='imagenet', include_top=False, input_shape=img_shape)\n","        base_model.trainable = True\n","        inputs = Input(shape=img_shape)\n","        x = base_model(inputs, training=False)\n","        x = tf.keras.layers.BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001)(x)\n","        x = GlobalAveragePooling2D()(x)\n","\n","        hp_units_1 = hp.Int('dense_units_1', min_value=512, max_value=2048, step=256, default=1024)\n","        hp_units_2 = hp.Int('dense_units_2', min_value=256, max_value=1024, step=128, default=512)\n","        hp_units_3 = hp.Int('dense_units_3', min_value=128, max_value=512, step=64, default=256)\n","\n","        hp_l2_1 = hp.Float('l2_1', min_value=1e-6, max_value=1e-2, sampling='log', default=1e-4)\n","        hp_l2_2 = hp.Float('l2_2', min_value=1e-6, max_value=1e-2, sampling='log', default=1e-4)\n","        hp_l2_3 = hp.Float('l2_3', min_value=1e-6, max_value=1e-2, sampling='log', default=1e-4)\n","        x = Dense(\n","            hp_units_1,\n","            activation='relu',\n","            kernel_regularizer=regularizers.l2(hp_l2_1)  \n","        )(x)\n","\n","        x = Dense(\n","            hp_units_2,\n","            activation='relu',\n","            kernel_regularizer=regularizers.l2(hp_l2_2)  \n","        )(x)\n","\n","        x = Dense(\n","            hp_units_3,\n","            activation='relu',\n","            kernel_regularizer=regularizers.l2(hp_l2_3)  \n","        )(x)\n","\n","        x = Dropout(hp.Float('dropout_rate', min_value=0.3, max_value=0.7, step=0.1, default=0.5))(x)\n","\n","        predictions = Dense(len(labels), activation='sigmoid')(x)\n","\n","        model = keras.Model(inputs=inputs, outputs=predictions)\n","        model.compile(optimizer=Adam(learning_rate=0.00001),\n","                  loss=\"binary_crossentropy\",\n","                  metrics=['accuracy',tf.keras.metrics.AUC(name=\"auc\",  multi_label=True,num_labels=len(labels)),\n","                             tf.keras.metrics.AUC(name=\"auc_roc\", curve=\"ROC\", multi_label=True,num_labels=len(labels)),\n","                            tf.keras.metrics.AUC(name=\"auc_pr\", curve=\"PR\", multi_label=True,num_labels=len(labels)),\n","                            tf.keras.metrics.Precision(name=\"precision\"),\n","                            tf.keras.metrics.Recall(name=\"recall\"),\n","                            F1Score(num_classes=len(labels),average='weighted',threshold=0.5),\n","                            HammingLoss(),\n","                            tfr.keras.metrics.MeanAveragePrecisionMetric(name=\"map\")])\n","        return model"]},{"cell_type":"code","execution_count":18,"metadata":{"execution":{"iopub.execute_input":"2023-09-07T06:58:34.815064Z","iopub.status.busy":"2023-09-07T06:58:34.814664Z","iopub.status.idle":"2023-09-07T06:58:34.827285Z","shell.execute_reply":"2023-09-07T06:58:34.826345Z","shell.execute_reply.started":"2023-09-07T06:58:34.815019Z"},"trusted":true},"outputs":[],"source":["img_shape=(380,380,3)"]},{"cell_type":"markdown","metadata":{},"source":["### Hyperparameter Tuning with Keras Tuner\n","\n","In this code a hyperparameter tuning process is demonstrated using the Keras Tuner library. The primary goal of hyperparameter tuning is to search for the optimal hyperparameter settings that can improve the performance of a deep learning model. Here's an explanation of the code:\n","\n","#### Keras Tuner Configuration\n","\n","- The code imports the `keras_tuner` library and sets up a `RandomSearch` tuner instance. The `RandomSearch` tuner is utilized to randomly explore different hyperparameter configurations based on a predefined objective.\n","\n","- A `model_builder` function is defined and specified as the function to construct deep learning models. This function is used by the tuner to create and evaluate models with various hyperparameters.\n","\n","#### Tuning Objective\n","\n","- The objective for hyperparameter tuning is defined using the `kt.Objective` class. In this case, the objective is \"val_f1_score,\" and the aim is to maximize this metric during the tuning process. The direction is set to \"max\" to indicate the goal of maximizing the objective.\n","\n","#### Maximum Number of Trials\n","\n","- The `max_trials` parameter is set to 4, indicating that the tuner will execute a maximum of 4 trials to explore different hyperparameter configurations.\n","\n","#### Tuning Directory\n","\n","- A directory named 'hyperparameter_tuning' is designated as the storage location for recording the results of the hyperparameter tuning process. This directory will be created in the current working directory.\n","\n","#### Project Name\n","\n","- The project is identified with the name 'custom_image_augmentation' to distinguish this specific hyperparameter tuning project.\n","\n","Hyperparameter tuning plays a critical role in optimizing the performance of deep learning models. The use of Keras Tuner automates the search for the best hyperparameters, making the tuning process more efficient and effective.\n","\n"]},{"cell_type":"code","execution_count":25,"metadata":{"execution":{"iopub.execute_input":"2023-09-07T07:00:40.991216Z","iopub.status.busy":"2023-09-07T07:00:40.990737Z","iopub.status.idle":"2023-09-07T07:00:51.244141Z","shell.execute_reply":"2023-09-07T07:00:51.243002Z","shell.execute_reply.started":"2023-09-07T07:00:40.991181Z"},"trusted":true},"outputs":[],"source":["import keras_tuner as kt  \n","tuner = RandomSearch(\n","    model_builder,\n","    objective=kt.Objective(\"val_f1_score\", direction=\"max\"), \n","    max_trials=4,  \n","    directory='hyperparameter_tuning',\n","    project_name='custom_image_augmentation'\n",")"]},{"cell_type":"code","execution_count":26,"metadata":{"execution":{"iopub.execute_input":"2023-09-07T07:00:51.246937Z","iopub.status.busy":"2023-09-07T07:00:51.246499Z","iopub.status.idle":"2023-09-07T07:00:51.255173Z","shell.execute_reply":"2023-09-07T07:00:51.254056Z","shell.execute_reply.started":"2023-09-07T07:00:51.246898Z"},"trusted":true},"outputs":[{"data":{"text/plain":["<keras_tuner.tuners.randomsearch.RandomSearch at 0x7f8bbe903730>"]},"execution_count":26,"metadata":{},"output_type":"execute_result"}],"source":["tuner"]},{"cell_type":"markdown","metadata":{},"source":["### Learning Rate Adjustment Callback\n","\n","In this code  a custom callback class named `LRA` is defined for learning rate adjustment during the training of a deep learning model. The primary purpose of this callback is to dynamically adjust the learning rate based on various conditions, optimizing the training process. Here's an explanation of the code:\n","\n","#### Callback Class\n","\n","- The `LRA` callback class is designed to adjust the learning rate during training. It accepts multiple parameters such as initial learning rate, patience, stop patience, threshold, factor, dwell, batches, initial epoch, and total epochs.\n","\n","#### Learning Rate Adjustment\n","\n","- The callback monitors training and validation metrics, such as accuracy, F1 score, validation loss, and more. Depending on specified criteria, the learning rate is adjusted accordingly.\n","\n","- The callback tracks factors like the improvement of validation accuracy, validation F1 score, and the reduction of validation loss.\n","\n","- If specified thresholds are not met or improvement is lacking, the callback reduces the learning rate and continues training. The process can be customized with dwell functionality to revert to better points in the optimization space.\n","\n","- If adjustments continue without substantial improvement within the defined stop patience, the training process is halted.\n","\n","The `LRA` callback offers dynamic control over the learning rate, ensuring that the deep learning model converges effectively and efficiently during training. It serves as a valuable tool for fine-tuning the model's performance.\n","\n","\n"]},{"cell_type":"code","execution_count":27,"metadata":{"execution":{"iopub.execute_input":"2023-09-07T07:00:51.258059Z","iopub.status.busy":"2023-09-07T07:00:51.257160Z","iopub.status.idle":"2023-09-07T07:00:51.292483Z","shell.execute_reply":"2023-09-07T07:00:51.291425Z","shell.execute_reply.started":"2023-09-07T07:00:51.257999Z"},"trusted":true},"outputs":[],"source":["\n","class LRA(keras.callbacks.Callback):\n","    def __init__(self,initial_lr, patience,stop_patience, threshold, factor, dwell, batches, initial_epoch,epochs):\n","        super(LRA, self).__init__()\n","        self.patience=patience \n","        self.stop_patience=stop_patience \n","        self.threshold=threshold \n","        self.factor=factor \n","        self.dwell=dwell\n","        self.batches=batches \n","        self.initial_epoch=initial_epoch\n","        self.epochs=epochs\n","        self.count=0 \n","        self.stop_count=0   \n","        self.highest_f1_score=0\n","        self.best_epoch=1         \n","        self.initial_lr=initial_lr         \n","        self.highest_tracc=0.0 \n","        self.lowest_vloss=np.inf \n","    def on_epoch_end(self, epoch, logs=None):  \n","        lr=float(tf.keras.backend.get_value(self.model.optimizer.lr)) \n","        current_lr=lr\n","        v_loss=logs.get('val_loss')  \n","        acc=logs.get('accuracy')  \n","        loss=logs.get('loss')\n","        auc=logs.get('auc')\n","        v_auc=logs.get('val_auc')\n","        Map=logs.get('map')\n","        val_map=logs.get('val_map')\n","        v_acc = logs.get('val_accuracy')\n","        val_precision = logs.get('val_precision')\n","        val_recall = logs.get('val_recall')\n","        val_f1_score = logs.get('val_f1_score')\n","        val_hamming_loss = logs.get('val_hamming_loss')\n","        precision = logs.get('precision')\n","        recall = logs.get('recall')\n","        f1_score = logs.get('f1_score')\n","        hamming_loss = logs.get('hamming_loss')\n","        auc_roc=logs.get('auc_roc')\n","        val_auc_roc=logs.get('val_auc_roc')\n","        auc_pr=logs.get('auc_pr')\n","        val_auc_pr=logs.get('val_auc_pr')\n","        if v_acc < self.threshold: \n","            monitor='val_accuracy'\n","            if epoch ==0:\n","                pimprov=0.0\n","            else:\n","                pimprov= (v_acc-self.highest_tracc )*100/self.highest_tracc\n","            if v_acc>self.highest_tracc: \n","                self.highest_tracc=v_acc \n","                self.best_weights=self.model.get_weights() \n","                self.count=0 \n","                self.stop_count=0 \n","                if v_loss<self.lowest_vloss:\n","                    self.lowest_vloss=v_loss\n","                if val_f1_score>self.highest_f1_score :\n","                    self.highest_f1_score = val_f1_score\n","                color= (0,255,0)\n","                self.best_epoch=epoch + 1              \n","            else: \n","                if self.count>=self.patience -1: \n","                    color=(245, 170, 66)\n","                    lr= lr* self.factor \n","                    tf.keras.backend.set_value(self.model.optimizer.lr, lr) \n","                    self.count=0 \n","                    self.stop_count=self.stop_count + 1 \n","                    self.count=0 # reset counter\n","                    if self.dwell:\n","                        self.model.set_weights(self.best_weights)                     \n","                    else:\n","                        if v_loss<self.lowest_vloss:\n","                            self.lowest_vloss=v_loss    \n","                        if val_f1_score>self.highest_f1_score :\n","                            self.highest_f1_score = val_f1_score\n","                else:\n","                    self.count=self.count +1   \n","        elif val_f1_score < 0.8:\n","            monitor='val_f1_score'\n","            if epoch ==0:\n","                pimprov=0.0\n","            else:\n","                pimprov= (val_f1_score-self.highest_f1_score )*100/self.highest_f1_score\n","            if val_f1_score>self.highest_f1_score: \n","                self.highest_f1_score=val_f1_score \n","                self.best_weights=self.model.get_weights() \n","                self.count=0 \n","                self.stop_count=0 \n","                if v_loss<self.lowest_vloss:\n","                    self.lowest_vloss=v_loss\n","                if v_acc>self.highest_tracc:\n","                    self.highest_tracc= v_acc\n","                color= (0,255,0)\n","                self.best_epoch=epoch + 1          \n","            else: \n","                if self.count>=self.patience -1: \n","                    color=(245, 170, 66)\n","                    lr= lr* self.factor \n","                    tf.keras.backend.set_value(self.model.optimizer.lr, lr) \n","                    self.count=0 \n","                    self.stop_count=self.stop_count + 1 \n","                    self.count=0 \n","                    if self.dwell:\n","                        self.model.set_weights(self.best_weights)                        \n","                    else:\n","                        if v_loss<self.lowest_vloss:\n","                            self.lowest_vloss=v_loss  \n","                        if v_acc>self.highest_tracc:\n","                            self.highest_tracc= v_acc\n","                else:\n","                    self.count=self.count +1           \n","        else: \n","            monitor='val_loss'\n","            if epoch ==0:\n","                pimprov=0.0\n","            else:\n","                pimprov= (self.lowest_vloss- v_loss )*100/self.lowest_vloss\n","            if v_loss< self.lowest_vloss: \n","                self.lowest_vloss=v_loss          \n","                self.best_weights=self.model.get_weights() \n","                self.count=0 \n","                self.stop_count=0  \n","                color=(0,255,0)                \n","                self.best_epoch=epoch + 1 \n","            else: \n","                if self.count>=self.patience-1: \n","                    color=(245, 170, 66)\n","                    lr=lr * self.factor                   \n","                    self.stop_count=self.stop_count + 1 \n","                    self.count=0 \n","                    tf.keras.backend.set_value(self.model.optimizer.lr, lr) \n","                    if self.dwell:\n","                        self.model.set_weights(self.best_weights) \n","                else: \n","                    self.count =self.count +1                    \n","                if v_acc>self.highest_tracc:\n","                    self.highest_tracc= v_acc\n","                if val_f1_score>self.highest_f1_score :\n","                    self.highest_f1_score = val_f1_score\n","        \n","        if self.stop_count> self.stop_patience - 1: \n","            msg=f' training has been halted at epoch {epoch + 1} after {self.stop_patience} adjustments of learning rate with no improvement'\n","            print_in_color(msg, (0,255,255), (55,65,80))\n","            self.model.stop_training = True "]},{"cell_type":"code","execution_count":28,"metadata":{"execution":{"iopub.execute_input":"2023-09-07T07:00:51.296255Z","iopub.status.busy":"2023-09-07T07:00:51.295114Z","iopub.status.idle":"2023-09-07T07:00:51.316954Z","shell.execute_reply":"2023-09-07T07:00:51.315884Z","shell.execute_reply.started":"2023-09-07T07:00:51.296217Z"},"trusted":true},"outputs":[{"data":{"text/plain":["235"]},"execution_count":28,"metadata":{},"output_type":"execute_result"}],"source":["train_steps=int(np.ceil(len(train_generator.labels)/batch_size))\n","train_steps"]},{"cell_type":"code","execution_count":29,"metadata":{"execution":{"iopub.execute_input":"2023-09-07T07:00:51.319142Z","iopub.status.busy":"2023-09-07T07:00:51.318540Z","iopub.status.idle":"2023-09-07T07:00:51.324089Z","shell.execute_reply":"2023-09-07T07:00:51.322958Z","shell.execute_reply.started":"2023-09-07T07:00:51.319107Z"},"trusted":true},"outputs":[],"source":["working_dir = '/kaggle/working/'"]},{"cell_type":"markdown","metadata":{},"source":["### Training Configuration Parameters\n","\n","This section outlines key parameters used to configure the training process of a deep learning model:\n","\n","#### `epochs`\n","\n","- `epochs`: The total number of training epochs. In this example, it is set to 20. Each epoch represents one complete iteration through the entire training dataset.\n","\n","#### `patience`\n","\n","- `patience`: This parameter determines the number of epochs to wait before considering adjustments to the learning rate if the monitored value (e.g., accuracy) does not improve. Here, it is set to 8.\n","\n","#### `stop_patience`\n","\n","- `stop_patience`: Specifies the number of epochs to wait before halting the training if the monitored value does not exhibit improvement. A value of 2 means that if the model's performance does not improve over the course of 2 consecutive epochs, the training process will be stopped.\n","\n","#### `threshold`\n","\n","- `threshold`: The threshold is a critical value that impacts which aspect of the model's performance is monitored. If the training accuracy falls below this threshold (0.65 in this example), the callback focuses on monitoring the training accuracy. If the training accuracy is equal to or exceeds this threshold, the callback switches to monitoring the validation loss.\n","\n","#### `factor`\n","\n","- `factor`: Represents the rate at which the learning rate is reduced when an adjustment is triggered. A factor of 0.1 signifies that the learning rate will be reduced to one-tenth of its previous value.\n","\n","#### `dwell`\n","\n","- `dwell`: When set to `True`, this parameter enables an experimental feature. If the monitored metric does not improve in the current epoch, the model's weights will be reset to those of the previous epoch, aiming to explore better training outcomes.\n","\n","#### `ask_epoch`\n","\n","- `ask_epoch`: Specifies the number of epochs to run before prompting the user or training process to inquire about halting training. It is set to 100 in this example.\n","\n","#### `batches`\n","\n","- `batches`: Defines the number of training batches to process per epoch. The specific value may depend on your dataset and hardware setup.\n","\n","#### `csv_path`\n","\n","- `csv_path`: Represents the path where the training-related CSV file will be saved. It's defined by joining the `working_dir` and 'my_csv'.\n","\n","These training configuration parameters are essential for controlling the training dynamics, ensuring efficient learning, and monitoring the model's performance.\n"]},{"cell_type":"code","execution_count":30,"metadata":{"execution":{"iopub.execute_input":"2023-09-07T07:00:51.326146Z","iopub.status.busy":"2023-09-07T07:00:51.325724Z","iopub.status.idle":"2023-09-07T07:00:51.335282Z","shell.execute_reply":"2023-09-07T07:00:51.334398Z","shell.execute_reply.started":"2023-09-07T07:00:51.326107Z"},"trusted":true},"outputs":[],"source":["epochs =20\n","patience= 8 \n","stop_patience =2 \n","threshold=.65 \n","factor=0.1 \n","dwell=True \n","ask_epoch=100 \n","batches=train_steps\n","csv_path=os.path.join(working_dir,'my_csv')\n"]},{"cell_type":"markdown","metadata":{},"source":["### Custom Colorful Text Printer\n","\n","In this code a custom text printer function called `print_in_color` is defined. This function enables the printing of text messages in a user-specified foreground color on a custom background color. Here's an explanation of the code:\n","\n","#### Custom Text Printer Function\n","\n","- The `print_in_color` function is designed to display text in a customized color scheme.\n","\n","- It accepts three parameters:\n","\n","  - `txt_msg`: The text message to be printed.\n","  - `fore_tupple`: A tuple representing the RGB values of the foreground color.\n","  - `back_tupple`: A tuple representing the RGB values of the background color.\n","\n","- Within the function, an ANSI escape code is constructed to specify the desired text and background colors using the RGB values provided in the tuples.\n","\n","- The `print` function is used to display the `txt_msg` with the specified colors.\n","\n","- After printing, a reset code (`'\\33[0m'`) is included to return the print color to the default black.\n","\n","This `print_in_color` function serves as a helpful tool for enhancing the visual appeal of text in code output or command line interfaces by incorporating custom colors.\n","\n"]},{"cell_type":"code","execution_count":31,"metadata":{"execution":{"iopub.execute_input":"2023-09-07T07:01:07.900220Z","iopub.status.busy":"2023-09-07T07:01:07.899813Z","iopub.status.idle":"2023-09-07T07:01:07.907781Z","shell.execute_reply":"2023-09-07T07:01:07.906420Z","shell.execute_reply.started":"2023-09-07T07:01:07.900187Z"},"trusted":true},"outputs":[],"source":["import time\n","import datetime\n","from datetime import datetime\n","def print_in_color(txt_msg,fore_tupple,back_tupple,):\n","\n","    rf,gf,bf=fore_tupple\n","    rb,gb,bb=back_tupple\n","    msg='{0}' + txt_msg\n","    mat='\\33[38;2;' + str(rf) +';' + str(gf) + ';' + str(bf) + ';48;2;' + str(rb) + ';' +str(gb) + ';' + str(bb) +'m' \n","    print(msg .format(mat), flush=True)\n","    print('\\33[0m', flush=True) \n","    return"]},{"cell_type":"markdown","metadata":{},"source":["### Custom Callback for Clearing Training Output\n","\n","In this code  a custom callback class named `ClearTrainingOutput` is defined. This callback is intended to clear the training output from the console window once the training is complete. Here's an explanation of the code:\n","\n","#### Custom Callback Class\n","\n","- The `ClearTrainingOutput` class extends the functionality of the `tf.keras.callbacks.Callback` class to define custom behavior when the training ends.\n","\n","- Within the `on_train_end` method, the code checks the operating system (OS) using `os.name`. If the OS is Windows (indicated by `'nt'`), the `cls` command is executed to clear the console. For non-Windows OS (e.g., Unix-like systems), the `clear` command is used for the same purpose.\n","\n","- This clearing action ensures that the console window is free from the training output, providing a cleaner and more organized environment.\n","\n","After defining this custom callback, it is used when conducting a hyperparameter search using the Keras Tuner.\n","\n"]},{"cell_type":"code","execution_count":32,"metadata":{"execution":{"iopub.execute_input":"2023-09-07T07:01:09.604529Z","iopub.status.busy":"2023-09-07T07:01:09.604119Z","iopub.status.idle":"2023-09-07T16:49:30.193066Z","shell.execute_reply":"2023-09-07T16:49:30.191342Z","shell.execute_reply.started":"2023-09-07T07:01:09.604497Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Trial 4 Complete [02h 25m 57s]\n","val_f1_score: 0.6413334608078003\n","\n","Best val_f1_score So Far: 0.771996021270752\n","Total elapsed time: 09h 48m 21s\n"]}],"source":["\n","import os\n","class ClearTrainingOutput(tf.keras.callbacks.Callback):\n","    def on_train_end(self, logs=None):\n","        os.system('cls' if os.name == 'nt' else 'clear')\n","tuner.search(\n","    train_generator,\n","    validation_data=val_generator,\n","    callbacks=[LRA(initial_lr=0.00001,patience=patience,stop_patience=stop_patience, threshold=threshold,\n","                   factor=factor,dwell=dwell, batches=batches,initial_epoch=0,epochs=epochs), ClearTrainingOutput()], epochs=epochs,\n","    validation_steps=None, shuffle=False, initial_epoch=0\n",")\n","\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["### Display Best Hyperparameters\n","\n","In this code the best hyperparameters obtained from the hyperparameter tuning process are retrieved and displayed. Here's an explanation of the code:\n","\n","#### Retrieving Best Hyperparameters\n","\n","- The `tuner.get_best_hyperparameters(num_trials=1)` method is used to obtain the best hyperparameters. With `num_trials=1`, we retrieve the top-performing hyperparameters.\n","\n","#### Displaying Best Hyperparameters\n","\n","- The code prints the best hyperparameters, including values for `Dense Units 1`, `Dense Units 2`, `Dense Units 3`, `L2 Regularization 1`, `L2 Regularization 2`, `L2 Regularization 3`, and `Dropout Rate`.\n","\n","- These hyperparameters are crucial for configuring the model architecture and training settings. Displaying them allows us to understand the optimal settings found during hyperparameter tuning.\n","\n","By showcasing the best hyperparameters, they can be used to build and train the final deep learning model with the most effective configuration.\n"]},{"cell_type":"code","execution_count":33,"metadata":{"execution":{"iopub.execute_input":"2023-09-07T16:49:30.198996Z","iopub.status.busy":"2023-09-07T16:49:30.198720Z","iopub.status.idle":"2023-09-07T16:49:30.207363Z","shell.execute_reply":"2023-09-07T16:49:30.204419Z","shell.execute_reply.started":"2023-09-07T16:49:30.198970Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Best Hyperparameters:\n","Dense Units 1: 1792\n","Dense Units 2: 768\n","Dense Units 3: 384\n","L2 Regularization 1: 3.060179690969728e-06\n","L2 Regularization 2: 0.0001487954829867211\n","L2 Regularization 3: 0.0006669665937898749\n","Dropout Rate: 0.5\n"]}],"source":["best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n","\n","print(\"Best Hyperparameters:\")\n","print(f\"Dense Units 1: {best_hps.get('dense_units_1')}\")\n","print(f\"Dense Units 2: {best_hps.get('dense_units_2')}\")\n","print(f\"Dense Units 3: {best_hps.get('dense_units_3')}\")\n","print(f\"L2 Regularization 1: {best_hps.get('l2_1')}\")\n","print(f\"L2 Regularization 2: {best_hps.get('l2_2')}\")\n","print(f\"L2 Regularization 3: {best_hps.get('l2_3')}\")\n","print(f\"Dropout Rate: {best_hps.get('dropout_rate')}\")"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":4}
