{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2023-10-08T06:02:38.334736Z","iopub.status.busy":"2023-10-08T06:02:38.334327Z","iopub.status.idle":"2023-10-08T06:02:48.995600Z","shell.execute_reply":"2023-10-08T06:02:48.994483Z","shell.execute_reply.started":"2023-10-08T06:02:38.334700Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting tensorflow-ranking==0.5.0\n","  Downloading tensorflow_ranking-0.5.0-py2.py3-none-any.whl (141 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.3/141.3 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: absl-py>=0.1.6 in /opt/conda/lib/python3.10/site-packages (from tensorflow-ranking==0.5.0) (1.4.0)\n","Requirement already satisfied: numpy>=1.13.3 in /opt/conda/lib/python3.10/site-packages (from tensorflow-ranking==0.5.0) (1.23.5)\n","Requirement already satisfied: six>=1.10.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow-ranking==0.5.0) (1.16.0)\n","Requirement already satisfied: tensorflow-serving-api<3.0.0,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow-ranking==0.5.0) (2.12.1)\n","Requirement already satisfied: grpcio<2.0,>=1.24.3 in /opt/conda/lib/python3.10/site-packages (from tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow-ranking==0.5.0) (1.51.1)\n","Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /opt/conda/lib/python3.10/site-packages (from tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow-ranking==0.5.0) (3.20.3)\n","Requirement already satisfied: tensorflow<3,>=2.12.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow-ranking==0.5.0) (2.12.0)\n","Requirement already satisfied: astunparse>=1.6.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow<3,>=2.12.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow-ranking==0.5.0) (1.6.3)\n","Requirement already satisfied: flatbuffers>=2.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow<3,>=2.12.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow-ranking==0.5.0) (23.5.26)\n","Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow<3,>=2.12.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow-ranking==0.5.0) (0.4.0)\n","Requirement already satisfied: google-pasta>=0.1.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow<3,>=2.12.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow-ranking==0.5.0) (0.2.0)\n","Requirement already satisfied: h5py>=2.9.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow<3,>=2.12.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow-ranking==0.5.0) (3.9.0)\n","Requirement already satisfied: jax>=0.3.15 in /opt/conda/lib/python3.10/site-packages (from tensorflow<3,>=2.12.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow-ranking==0.5.0) (0.4.13)\n","Requirement already satisfied: keras<2.13,>=2.12.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow<3,>=2.12.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow-ranking==0.5.0) (2.12.0)\n","Requirement already satisfied: libclang>=13.0.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow<3,>=2.12.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow-ranking==0.5.0) (16.0.0)\n","Requirement already satisfied: opt-einsum>=2.3.2 in /opt/conda/lib/python3.10/site-packages (from tensorflow<3,>=2.12.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow-ranking==0.5.0) (3.3.0)\n","Requirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from tensorflow<3,>=2.12.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow-ranking==0.5.0) (21.3)\n","Requirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from tensorflow<3,>=2.12.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow-ranking==0.5.0) (68.0.0)\n","Requirement already satisfied: tensorboard<2.13,>=2.12 in /opt/conda/lib/python3.10/site-packages (from tensorflow<3,>=2.12.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow-ranking==0.5.0) (2.12.3)\n","Requirement already satisfied: tensorflow-estimator<2.13,>=2.12.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow<3,>=2.12.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow-ranking==0.5.0) (2.12.0)\n","Requirement already satisfied: termcolor>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow<3,>=2.12.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow-ranking==0.5.0) (2.3.0)\n","Requirement already satisfied: typing-extensions>=3.6.6 in /opt/conda/lib/python3.10/site-packages (from tensorflow<3,>=2.12.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow-ranking==0.5.0) (4.6.3)\n","Requirement already satisfied: wrapt<1.15,>=1.11.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow<3,>=2.12.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow-ranking==0.5.0) (1.14.1)\n","Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow<3,>=2.12.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow-ranking==0.5.0) (0.32.0)\n","Requirement already satisfied: wheel<1.0,>=0.23.0 in /opt/conda/lib/python3.10/site-packages (from astunparse>=1.6.0->tensorflow<3,>=2.12.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow-ranking==0.5.0) (0.40.0)\n","Requirement already satisfied: ml-dtypes>=0.1.0 in /opt/conda/lib/python3.10/site-packages (from jax>=0.3.15->tensorflow<3,>=2.12.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow-ranking==0.5.0) (0.2.0)\n","Requirement already satisfied: scipy>=1.7 in /opt/conda/lib/python3.10/site-packages (from jax>=0.3.15->tensorflow<3,>=2.12.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow-ranking==0.5.0) (1.11.2)\n","Requirement already satisfied: google-auth<3,>=1.6.3 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.13,>=2.12->tensorflow<3,>=2.12.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow-ranking==0.5.0) (2.20.0)\n","Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.13,>=2.12->tensorflow<3,>=2.12.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow-ranking==0.5.0) (1.0.0)\n","Requirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.13,>=2.12->tensorflow<3,>=2.12.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow-ranking==0.5.0) (3.4.3)\n","Requirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.13,>=2.12->tensorflow<3,>=2.12.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow-ranking==0.5.0) (2.31.0)\n","Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.13,>=2.12->tensorflow<3,>=2.12.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow-ranking==0.5.0) (0.7.1)\n","Requirement already satisfied: werkzeug>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.13,>=2.12->tensorflow<3,>=2.12.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow-ranking==0.5.0) (2.3.7)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->tensorflow<3,>=2.12.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow-ranking==0.5.0) (3.0.9)\n","Requirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow<3,>=2.12.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow-ranking==0.5.0) (4.2.4)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow<3,>=2.12.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow-ranking==0.5.0) (0.2.7)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow<3,>=2.12.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow-ranking==0.5.0) (4.9)\n","Requirement already satisfied: urllib3<2.0 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow<3,>=2.12.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow-ranking==0.5.0) (1.26.15)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow<3,>=2.12.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow-ranking==0.5.0) (1.3.1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow<3,>=2.12.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow-ranking==0.5.0) (3.1.0)\n","Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow<3,>=2.12.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow-ranking==0.5.0) (3.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow<3,>=2.12.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow-ranking==0.5.0) (2023.7.22)\n","Requirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard<2.13,>=2.12->tensorflow<3,>=2.12.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow-ranking==0.5.0) (2.1.3)\n","Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow<3,>=2.12.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow-ranking==0.5.0) (0.4.8)\n","Requirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.10/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow<3,>=2.12.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow-ranking==0.5.0) (3.2.2)\n","Installing collected packages: tensorflow-ranking\n","Successfully installed tensorflow-ranking-0.5.0\n"]}],"source":["!pip install tensorflow-ranking==0.5.0"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2023-10-08T06:02:49.007286Z","iopub.status.busy":"2023-10-08T06:02:49.000883Z","iopub.status.idle":"2023-10-08T06:03:56.571149Z","shell.execute_reply":"2023-10-08T06:03:56.569944Z","shell.execute_reply.started":"2023-10-08T06:02:49.007241Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting tensorflow==2.9.1\n","  Downloading tensorflow-2.9.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (511.7 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m511.7/511.7 MB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n","\u001b[?25hRequirement already satisfied: absl-py>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.9.1) (1.4.0)\n","Requirement already satisfied: astunparse>=1.6.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.9.1) (1.6.3)\n","Collecting flatbuffers<2,>=1.12 (from tensorflow==2.9.1)\n","  Downloading flatbuffers-1.12-py2.py3-none-any.whl (15 kB)\n","Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.9.1) (0.4.0)\n","Requirement already satisfied: google-pasta>=0.1.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.9.1) (0.2.0)\n","Requirement already satisfied: grpcio<2.0,>=1.24.3 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.9.1) (1.51.1)\n","Requirement already satisfied: h5py>=2.9.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.9.1) (3.9.0)\n","Collecting keras<2.10.0,>=2.9.0rc0 (from tensorflow==2.9.1)\n","  Downloading keras-2.9.0-py2.py3-none-any.whl (1.6 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n","\u001b[?25hCollecting keras-preprocessing>=1.1.1 (from tensorflow==2.9.1)\n","  Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.6/42.6 kB\u001b[0m \u001b[31m703.1 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n","\u001b[?25hRequirement already satisfied: libclang>=13.0.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.9.1) (16.0.0)\n","Requirement already satisfied: numpy>=1.20 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.9.1) (1.23.5)\n","Requirement already satisfied: opt-einsum>=2.3.2 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.9.1) (3.3.0)\n","Requirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.9.1) (21.3)\n","Collecting protobuf<3.20,>=3.9.2 (from tensorflow==2.9.1)\n","  Downloading protobuf-3.19.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m\n","\u001b[?25hRequirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.9.1) (68.0.0)\n","Requirement already satisfied: six>=1.12.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.9.1) (1.16.0)\n","Collecting tensorboard<2.10,>=2.9 (from tensorflow==2.9.1)\n","  Downloading tensorboard-2.9.1-py3-none-any.whl (5.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.8/5.8 MB\u001b[0m \u001b[31m71.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n","\u001b[?25hRequirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.9.1) (0.32.0)\n","Collecting tensorflow-estimator<2.10.0,>=2.9.0rc0 (from tensorflow==2.9.1)\n","  Downloading tensorflow_estimator-2.9.0-py2.py3-none-any.whl (438 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m438.7/438.7 kB\u001b[0m \u001b[31m34.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: termcolor>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.9.1) (2.3.0)\n","Requirement already satisfied: typing-extensions>=3.6.6 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.9.1) (4.6.3)\n","Requirement already satisfied: wrapt>=1.11.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.9.1) (1.14.1)\n","Requirement already satisfied: wheel<1.0,>=0.23.0 in /opt/conda/lib/python3.10/site-packages (from astunparse>=1.6.0->tensorflow==2.9.1) (0.40.0)\n","Requirement already satisfied: google-auth<3,>=1.6.3 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.10,>=2.9->tensorflow==2.9.1) (2.20.0)\n","Collecting google-auth-oauthlib<0.5,>=0.4.1 (from tensorboard<2.10,>=2.9->tensorflow==2.9.1)\n","  Downloading google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\n","Requirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.10,>=2.9->tensorflow==2.9.1) (3.4.3)\n","Requirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.10,>=2.9->tensorflow==2.9.1) (2.31.0)\n","Collecting tensorboard-data-server<0.7.0,>=0.6.0 (from tensorboard<2.10,>=2.9->tensorflow==2.9.1)\n","  Downloading tensorboard_data_server-0.6.1-py3-none-manylinux2010_x86_64.whl (4.9 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m75.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n","\u001b[?25hCollecting tensorboard-plugin-wit>=1.6.0 (from tensorboard<2.10,>=2.9->tensorflow==2.9.1)\n","  Downloading tensorboard_plugin_wit-1.8.1-py3-none-any.whl (781 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m781.3/781.3 kB\u001b[0m \u001b[31m52.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: werkzeug>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.10,>=2.9->tensorflow==2.9.1) (2.3.7)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->tensorflow==2.9.1) (3.0.9)\n","Requirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow==2.9.1) (4.2.4)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow==2.9.1) (0.2.7)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow==2.9.1) (4.9)\n","Requirement already satisfied: urllib3<2.0 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow==2.9.1) (1.26.15)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow==2.9.1) (1.3.1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow==2.9.1) (3.1.0)\n","Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow==2.9.1) (3.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow==2.9.1) (2023.7.22)\n","Requirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard<2.10,>=2.9->tensorflow==2.9.1) (2.1.3)\n","Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow==2.9.1) (0.4.8)\n","Requirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.10/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow==2.9.1) (3.2.2)\n","Installing collected packages: tensorboard-plugin-wit, keras, flatbuffers, tensorflow-estimator, tensorboard-data-server, protobuf, keras-preprocessing, google-auth-oauthlib, tensorboard, tensorflow\n","  Attempting uninstall: keras\n","    Found existing installation: keras 2.12.0\n","    Uninstalling keras-2.12.0:\n","      Successfully uninstalled keras-2.12.0\n","  Attempting uninstall: flatbuffers\n","    Found existing installation: flatbuffers 23.5.26\n","    Uninstalling flatbuffers-23.5.26:\n","      Successfully uninstalled flatbuffers-23.5.26\n","  Attempting uninstall: tensorflow-estimator\n","    Found existing installation: tensorflow-estimator 2.12.0\n","    Uninstalling tensorflow-estimator-2.12.0:\n","      Successfully uninstalled tensorflow-estimator-2.12.0\n","  Attempting uninstall: tensorboard-data-server\n","    Found existing installation: tensorboard-data-server 0.7.1\n","    Uninstalling tensorboard-data-server-0.7.1:\n","      Successfully uninstalled tensorboard-data-server-0.7.1\n","  Attempting uninstall: protobuf\n","    Found existing installation: protobuf 3.20.3\n","    Uninstalling protobuf-3.20.3:\n","      Successfully uninstalled protobuf-3.20.3\n","  Attempting uninstall: google-auth-oauthlib\n","    Found existing installation: google-auth-oauthlib 1.0.0\n","    Uninstalling google-auth-oauthlib-1.0.0:\n","      Successfully uninstalled google-auth-oauthlib-1.0.0\n","  Attempting uninstall: tensorboard\n","    Found existing installation: tensorboard 2.12.3\n","    Uninstalling tensorboard-2.12.3:\n","      Successfully uninstalled tensorboard-2.12.3\n","  Attempting uninstall: tensorflow\n","    Found existing installation: tensorflow 2.12.0\n","    Uninstalling tensorflow-2.12.0:\n","      Successfully uninstalled tensorflow-2.12.0\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","cudf 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\n","cuml 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\n","dask-cudf 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\n","apache-beam 2.46.0 requires dill<0.3.2,>=0.3.1.1, but you have dill 0.3.7 which is incompatible.\n","apache-beam 2.46.0 requires pyarrow<10.0.0,>=3.0.0, but you have pyarrow 11.0.0 which is incompatible.\n","cudf 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.0.2 which is incompatible.\n","cudf 23.8.0 requires protobuf<5,>=4.21, but you have protobuf 3.19.6 which is incompatible.\n","cuml 23.8.0 requires dask==2023.7.1, but you have dask 2023.9.0 which is incompatible.\n","dask-cudf 23.8.0 requires dask==2023.7.1, but you have dask 2023.9.0 which is incompatible.\n","dask-cudf 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.0.2 which is incompatible.\n","gcsfs 2023.6.0 requires fsspec==2023.6.0, but you have fsspec 2023.9.0 which is incompatible.\n","google-cloud-aiplatform 0.6.0a1 requires google-api-core[grpc]<2.0.0dev,>=1.22.2, but you have google-api-core 2.11.1 which is incompatible.\n","google-cloud-automl 1.0.1 requires google-api-core[grpc]<2.0.0dev,>=1.14.0, but you have google-api-core 2.11.1 which is incompatible.\n","google-cloud-pubsub 2.17.1 requires grpcio<2.0dev,>=1.51.3, but you have grpcio 1.51.1 which is incompatible.\n","kfp 2.0.1 requires google-cloud-storage<3,>=2.2.1, but you have google-cloud-storage 1.44.0 which is incompatible.\n","onnx 1.14.1 requires protobuf>=3.20.2, but you have protobuf 3.19.6 which is incompatible.\n","tensorflow-datasets 4.9.2 requires protobuf>=3.20, but you have protobuf 3.19.6 which is incompatible.\n","tensorflow-decision-forests 1.4.0 requires tensorflow~=2.12.0, but you have tensorflow 2.9.1 which is incompatible.\n","tensorflow-serving-api 2.12.1 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3, but you have protobuf 3.19.6 which is incompatible.\n","tensorflow-serving-api 2.12.1 requires tensorflow<3,>=2.12.0, but you have tensorflow 2.9.1 which is incompatible.\n","tensorflow-text 2.12.1 requires tensorflow<2.13,>=2.12.0; platform_machine != \"arm64\" or platform_system != \"Darwin\", but you have tensorflow 2.9.1 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed flatbuffers-1.12 google-auth-oauthlib-0.4.6 keras-2.9.0 keras-preprocessing-1.1.2 protobuf-3.19.6 tensorboard-2.9.1 tensorboard-data-server-0.6.1 tensorboard-plugin-wit-1.8.1 tensorflow-2.9.1 tensorflow-estimator-2.9.0\n"]}],"source":["!pip install tensorflow==2.9.1"]},{"cell_type":"markdown","metadata":{},"source":["**Importing  libraries**"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2023-10-08T06:03:56.577497Z","iopub.status.busy":"2023-10-08T06:03:56.577098Z","iopub.status.idle":"2023-10-08T06:04:02.297077Z","shell.execute_reply":"2023-10-08T06:04:02.296110Z","shell.execute_reply.started":"2023-10-08T06:03:56.577456Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n","  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","/opt/conda/lib/python3.10/site-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: \n","\n","TensorFlow Addons (TFA) has ended development and introduction of new features.\n","TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n","Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n","\n","For more information see: https://github.com/tensorflow/addons/issues/2807 \n","\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/tensorflow_addons/utils/ensure_tf_install.py:53: UserWarning: Tensorflow Addons supports using Python ops for all Tensorflow versions above or equal to 2.11.0 and strictly below 2.14.0 (nightly versions are not supported). \n"," The versions of TensorFlow you are currently using is 2.9.1 and is not supported. \n","Some things might work, some things might not.\n","If you were to encounter a bug, do not file an issue.\n","If you want to make sure you're using a tested and supported configuration, either change the TensorFlow version or the TensorFlow Addons's version. \n","You can find the compatibility matrix in TensorFlow Addon's readme:\n","https://github.com/tensorflow/addons\n","  warnings.warn(\n","/tmp/ipykernel_28/4031221383.py:15: DeprecationWarning: `import kerastuner` is deprecated, please use `import keras_tuner`.\n","  from kerastuner.tuners import RandomSearch\n"]}],"source":["import pandas as pd\n","import os\n","import numpy as np\n","import pandas as pd\n","from tensorflow import keras\n","from tensorflow.keras.optimizers import Adam\n","from tensorflow.keras.applications.inception_v3 import InceptionV3\n","from tensorflow_addons.metrics import F1Score\n","from tensorflow.keras.layers import Input, Dense, Dropout, GlobalAveragePooling2D\n","from tensorflow.keras import regularizers \n","from kerastuner.tuners import RandomSearch\n","import tensorflow_ranking as tfr"]},{"cell_type":"markdown","metadata":{},"source":["### Custom Hamming Loss Metric\n","\n","In this code  a custom Hamming Loss metric is defined using TensorFlow and Keras. The Hamming Loss is a metric used to evaluate the accuracy of multi-label classification models. Here's an explanation of the code:\n","\n","#### Custom Metric Class\n","\n","- A custom metric class named `HammingLoss` is defined. This class extends the `Metric` class provided by TensorFlow/Keras.\n","\n","- The `__init__` method initializes the metric. It accepts parameters such as `threshold` and `name`. The threshold defines the threshold value for binary conversion of predictions. The `hamming_loss` and `count` variables are created as TensorFlow variables to keep track of the Hamming loss and the number of samples.\n","\n","#### `update_state` Method\n","\n","- The `update_state` method is used to update the state of the metric. It accepts `y_true` (true labels), `y_pred` (predicted probabilities), and `sample_weight` (optional).\n","\n","- The method first converts the predicted probabilities to binary labels based on the specified threshold.\n","\n","- It then computes the absolute differences between the true labels and the binary predictions for each sample and class.\n","\n","- The mean over classes for each sample is calculated, which represents the Hamming loss for that sample.\n","\n","- The Hamming loss and the sample count are updated accordingly.\n","\n","#### `result` Method\n","\n","- The `result` method calculates the final Hamming loss by dividing the accumulated Hamming loss by the sample count.\n","\n","#### `reset_state` Method\n","\n","- The `reset_state` method is used to reset the Hamming loss and sample count at the end of each epoch. This is important to ensure that the metric calculations are isolated for each epoch.\n","\n","This custom Hamming Loss metric can be used during the training of multi-label classification models to monitor and evaluate the model's performance with regard to label prediction accuracy.\n","\n","\n"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2023-10-08T06:04:02.300981Z","iopub.status.busy":"2023-10-08T06:04:02.299630Z","iopub.status.idle":"2023-10-08T06:04:02.309588Z","shell.execute_reply":"2023-10-08T06:04:02.308616Z","shell.execute_reply.started":"2023-10-08T06:04:02.300941Z"},"trusted":true},"outputs":[],"source":["import tensorflow as tf\n","from tensorflow.keras.metrics import Metric\n","\n","class HammingLoss(Metric):\n","    def __init__(self, threshold=0.5, name=\"hamming_loss\", **kwargs):\n","        super(HammingLoss, self).__init__(name=name, **kwargs)\n","        self.threshold = tf.Variable(threshold, trainable=False, dtype=tf.float32)\n","        self.hamming_loss = self.add_weight(name=\"hl\", initializer=\"zeros\")\n","        self.count = self.add_weight(name=\"count\", initializer=\"zeros\")\n","\n","    def update_state(self, y_true, y_pred, sample_weight=None):\n","        y_pred_binary = tf.cast(y_pred > self.threshold, tf.float32)\n","        y_true = tf.cast(y_true, tf.float32)\n","        tmp = tf.math.abs(y_true - y_pred_binary)\n","        hl = tf.math.reduce_mean(tmp, axis=-1)\n","        self.hamming_loss.assign_add(tf.math.reduce_sum(hl))\n","        self.count.assign_add(tf.cast(tf.size(y_true) / tf.shape(y_true)[-1], tf.float32))\n","    def result(self):\n","        return self.hamming_loss / self.count\n","    def reset_state(self):\n","        self.hamming_loss.assign(0.)\n","        self.count.assign(0.)"]},{"cell_type":"markdown","metadata":{},"source":["### Data Loading and Filtering\n","\n","This section covers the process of loading data from CSV files and applying filtering to include only specific rows based on the value of the `Disease_Risk` column.\n","\n","- The data is loaded from the following CSV files:\n","  - Training data: `train_file.csv`\n","  - Validation data: `val_file.csv`\n","  - Test data: `test_file.csv`\n","\n","- To ensure that the analysis focuses on samples relevant to disease risk, a filter is applied to select rows where the `Disease_Risk` column is equal to 1. This step helps exclude unrelated data.\n","\n","- Additionally, the disease labels are extracted into the `labels` variable for further reference.\n","\n","- Finally, information is provided on the sizes of the resulting datasets, including the number of samples in the training, validation, and test sets.\n","\n","This data preparation process ensures that subsequent analysis or model training is based on the most relevant samples with disease risk.\n"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2023-10-08T06:04:02.312228Z","iopub.status.busy":"2023-10-08T06:04:02.311277Z","iopub.status.idle":"2023-10-08T06:04:02.573227Z","shell.execute_reply":"2023-10-08T06:04:02.572217Z","shell.execute_reply.started":"2023-10-08T06:04:02.312169Z"},"trusted":true},"outputs":[{"data":{"text/plain":["(7502, 934, 953, 9389)"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["train_data = pd.read_csv('/kaggle/input/my-data/train_file.csv')\n","val_data = pd.read_csv('/kaggle/input/my-data/val_file.csv')\n","test_data = pd.read_csv('/kaggle/input/my-data/test_file.csv')\n","train_data = train_data[train_data['Disease_Risk'] == 1]\n","val_data = val_data[val_data['Disease_Risk'] == 1]\n","test_data = test_data[test_data['Disease_Risk'] == 1]\n","labels = train_data.columns[2:-1]\n","len_train_data = len(train_data)\n","len_val_data = len(val_data)\n","len_test_data = len(test_data)\n","filtered_size = len_train_data + len_val_data + len_test_data\n","\n","len(train_data), len(val_data), len(test_data),filtered_size"]},{"cell_type":"markdown","metadata":{},"source":["### Custom Image Augmentation Layer\n","\n","In the following code  a custom image augmentation layer is defined using TensorFlow and Keras. This custom layer is designed to apply various image augmentation operations for data preprocessing and augmentation in deep learning models. Here's an explanation of the code:\n","\n","#### Custom Image Augmentation Class\n","\n","- The code defines a class called `CustomImageAugmentation` that inherits from `tf.keras.layers.Layer`. This class serves as the foundation for applying image augmentation techniques to input images.\n","\n","- The class constructor (`__init__`) is responsible for initializing the image augmentation layer. It accepts a set of parameters that allow customization of which augmentation operations are applied. These operations include horizontal flipping, rotation, brightness adjustment, contrast adjustment, saturation adjustment, hue adjustment, scaling, cropping, grid distortion, compression, Gaussian noise, Gaussian blur, downscaling, gamma correction, and elastic transformation.\n","\n","- The `call` method within this class is used to apply the specified augmentation operations to input images. It includes a parameter named `apply` that controls whether the augmentations should be applied or not.\n","\n","- The augmentation operations are diverse and encompass random horizontal flipping, random rotation, random brightness, contrast, saturation, and hue adjustments, scaling, and more. The application of each operation is controlled by the corresponding class attribute, such as `self.flip` and `self.rotate`.\n","\n","- If the `apply` parameter is set to `True`, the augmentation operations are applied to the input images; otherwise, the original images are returned.\n","\n","- The `img_aug` variable represents an instance of the `CustomImageAugmentation` class, which can be used for data augmentation within image-based deep learning models.\n","\n","This custom image augmentation layer offers flexibility in specifying and applying image transformations, enhancing model generalization and performance when working with image datasets.\n"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2023-10-08T06:04:02.575867Z","iopub.status.busy":"2023-10-08T06:04:02.574846Z","iopub.status.idle":"2023-10-08T06:04:02.589095Z","shell.execute_reply":"2023-10-08T06:04:02.588124Z","shell.execute_reply.started":"2023-10-08T06:04:02.575831Z"},"trusted":true},"outputs":[],"source":["import tensorflow as tf\n","class CustomImageAugmentation(tf.keras.layers.Layer):\n","    def __init__(self, flip=True, rotate=True, brightness=True,\n","                 contrast=True, saturation=True, hue=True, scale=False,\n","                 crop=False, grid_distortion=False, compression=False,\n","                 gaussian_noise=False, gaussian_blur=False,\n","                 downscaling=False, gamma=False, elastic_transform=False, **kwargs):\n","        super(CustomImageAugmentation, self).__init__(**kwargs)\n","        self.flip = flip\n","        self.rotate = rotate\n","        self.brightness = brightness\n","        self.contrast = contrast\n","        self.saturation = saturation\n","        self.hue = hue\n","        self.scale = scale\n","        self.crop = crop\n","        self.grid_distortion = grid_distortion\n","        self.compression = compression\n","        self.gaussian_noise = gaussian_noise\n","        self.gaussian_blur = gaussian_blur\n","        self.downscaling = downscaling\n","        self.gamma = gamma\n","        self.elastic_transform = elastic_transform\n","\n","    def call(self, inputs, apply=True):\n","        if apply:\n","            augmented = tf.image.random_flip_left_right(inputs) if self.flip else inputs\n","            augmented = tf.image.rot90(augmented, k=tf.random.uniform(shape=[], minval=0, maxval=4, dtype=tf.int32)) if self.rotate else augmented\n","            augmented = tf.image.random_brightness(augmented, max_delta=0.2) if self.brightness else augmented\n","            augmented = tf.image.random_contrast(augmented, lower=0.5, upper=1.5) if self.contrast else augmented\n","            augmented = tf.image.random_saturation(augmented, lower=0.5, upper=1.5) if self.saturation else augmented\n","            augmented = tf.image.random_hue(augmented, max_delta=0.2) if self.hue else augmented\n","            return augmented\n","        else:\n","            return inputs\n","img_aug = CustomImageAugmentation()"]},{"cell_type":"markdown","metadata":{},"source":["### Custom Image Data Generator for Augmentation\n","\n","In this code a custom image data generator is defined using TensorFlow and Keras. This data generator is designed to apply various image augmentation operations to preprocess and augment data in deep learning models. Here's an explanation of the code:\n","\n","#### Custom Image Data Generator Class\n","\n","- A custom image data generator class is created using TensorFlow and Keras.\n","\n","- The data generator is configured with various augmentation operations that can be applied during data preprocessing.\n","\n","- The `preprocessing_function` parameter is defined as a lambda function that utilizes the `img_aug` function with the `apply=True` flag to apply augmentation operations.\n","\n","- The defined data generator can be used to preprocess and augment images during the training of deep learning models, enhancing the model's ability to learn from diverse and augmented data.\n","\n","\n"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2023-10-08T06:04:02.591111Z","iopub.status.busy":"2023-10-08T06:04:02.590443Z","iopub.status.idle":"2023-10-08T06:04:02.607035Z","shell.execute_reply":"2023-10-08T06:04:02.606169Z","shell.execute_reply.started":"2023-10-08T06:04:02.591077Z"},"trusted":true},"outputs":[],"source":["from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","\n","datagen = ImageDataGenerator(\n","    preprocessing_function=lambda x: img_aug(x, apply=True)\n",")"]},{"cell_type":"markdown","metadata":{},"source":["### Data Generator for Training\n","\n","In this code a data generator for training deep learning models is defined using TensorFlow and Keras. The data generator is configured to preprocess and augment the training data. Here's an explanation of the code:\n","\n","#### Data Generator Configuration\n","\n","- The data generator is configured with the specified `target_size` of (299, 299), which defines the desired size of input images.\n","\n","- The `batch_size` is set to 32, determining the number of samples to process in each batch during training.\n","\n","#### Data Flow Configuration\n","\n","- The `train_generator` is created using the `datagen.flow_from_dataframe` method.\n","\n","- It is associated with the training data stored in the `train_data` DataFrame.\n","\n","- The `x_col` parameter specifies the column name in the DataFrame where image file paths are stored.\n","\n","- The `y_col` parameter is set to the list of column names representing the labels in the DataFrame. These columns are obtained from `train_data.columns[2:-1].tolist()`.\n","\n","- The `class_mode` is set to 'raw', indicating that the generator should return raw arrays as the target values.\n","\n","- Images are processed in batches of size `batch_size`.\n","\n","- The `target_size` parameter is set to the specified dimensions of (299, 299) for image resizing.\n","\n","- Data shuffling is enabled with the `shuffle` parameter set to `True`.\n","\n","This data generator is essential for efficiently feeding training data to deep learning models, enabling data augmentation and resizing to match model input requirements.\n"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2023-10-08T06:04:02.609236Z","iopub.status.busy":"2023-10-08T06:04:02.608255Z","iopub.status.idle":"2023-10-08T06:04:11.116774Z","shell.execute_reply":"2023-10-08T06:04:11.115760Z","shell.execute_reply.started":"2023-10-08T06:04:02.609183Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Found 7502 validated image filenames.\n"]}],"source":["target_size=(299, 299)\n","batch_size=32\n","train_generator = datagen.flow_from_dataframe(\n","    dataframe=train_data,\n","    x_col='IMG_DIR',\n","    y_col=labels,\n","    class_mode='raw',\n","    batch_size=batch_size,\n","    target_size=target_size,\n","    shuffle=True\n",")"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2023-10-08T06:04:11.119134Z","iopub.status.busy":"2023-10-08T06:04:11.118114Z","iopub.status.idle":"2023-10-08T06:04:12.149696Z","shell.execute_reply":"2023-10-08T06:04:12.148099Z","shell.execute_reply.started":"2023-10-08T06:04:11.119095Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Found 934 validated image filenames.\n"]}],"source":["val_generator = datagen.flow_from_dataframe(\n","    dataframe=val_data,\n","    x_col='IMG_DIR',\n","    y_col=labels,\n","    class_mode='raw',\n","    batch_size=batch_size,\n","    target_size=target_size,\n","    shuffle=False  \n",")"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2023-10-08T06:04:12.157895Z","iopub.status.busy":"2023-10-08T06:04:12.156069Z","iopub.status.idle":"2023-10-08T06:04:15.841005Z","shell.execute_reply":"2023-10-08T06:04:15.840004Z","shell.execute_reply.started":"2023-10-08T06:04:12.157851Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Batch images shape: (32, 299, 299, 3)\n","Batch labels shape: (32, 28)\n"]}],"source":["for batch_images, batch_labels in train_generator:\n","    print(\"Batch images shape:\", batch_images.shape)\n","    print(\"Batch labels shape:\", batch_labels.shape)\n","    break"]},{"cell_type":"markdown","metadata":{},"source":["### Hyperparameter-Tuned Model Builder\n","\n","In this code a model builder is defined for hyperparameter tuning of an image classification model. The model architecture is based on the InceptionV3 with custom dense layers and various hyperparameters.\n","\n","#### Model Architecture\n","\n","- The code utilizes TensorFlow's Keras API to create a neural network model for image classification.\n","- The base model used is InceptionV3, pre-trained on ImageNet, with the option to fine-tune its layers.\n","- Global Average Pooling (GAP) is applied to the output of the base model to reduce the spatial dimensions.\n","- The architecture of the dense layers, their units, and L2 regularization are hyperparameters tuned during the process.\n","- The model includes three dense layers, each with customizable units and L2 regularization terms.\n","- A Dropout layer with a tunable rate is introduced for regularization.\n","- The output layer consists of multiple neurons, typically equal to the number of classes (specified as 'len(labels)'). A sigmoid activation function is used for multi-label classification.\n","\n","#### Hyperparameter Tuning\n","\n","- The `model_builder` function is designed to be used with a hyperparameter tuning framework (e.g., Keras Tuner).\n","- The function takes hyperparameters as inputs, allowing the architecture and regularization to be customized during the tuning process.\n","- Various hyperparameters, such as dense units, L2 regularization, and dropout rate, are declared and tunable within the function.\n","\n","#### Compilation\n","\n","- The model is compiled with the Adam optimizer and specific learning rate.\n","- Binary cross-entropy is chosen as the loss function, as it is suitable for binary multi-label classification.\n","- Multiple metrics are defined, including accuracy, AUC, precision, recall, F1-score, Hamming loss, and Mean Average Precision (mAP), suitable for multi-label classification problems.\n","\n","This code provides a flexible and tunable model builder for multi-label image classification tasks, allowing for the optimization of various architectural and regularization hyperparameters.\n"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2023-10-08T06:04:15.843282Z","iopub.status.busy":"2023-10-08T06:04:15.842574Z","iopub.status.idle":"2023-10-08T06:04:15.858886Z","shell.execute_reply":"2023-10-08T06:04:15.857939Z","shell.execute_reply.started":"2023-10-08T06:04:15.843244Z"},"trusted":true},"outputs":[],"source":["def model_builder(hp):\n","    strategy = tf.distribute.MirroredStrategy()\n","    with strategy.scope():\n","        base_model = InceptionV3(weights='imagenet', include_top=False, input_shape=img_shape)\n","        base_model.trainable = True\n","        inputs = Input(shape=img_shape)\n","        x = base_model(inputs, training=False)\n","        x = tf.keras.layers.BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001)(x)\n","        x = GlobalAveragePooling2D()(x)\n","\n","\n","        hp_units_1 = hp.Int('dense_units_1', min_value=512, max_value=2048, step=256, default=1024)\n","        hp_units_2 = hp.Int('dense_units_2', min_value=256, max_value=1024, step=128, default=512)\n","        hp_units_3 = hp.Int('dense_units_3', min_value=128, max_value=512, step=64, default=256)\n","\n","        hp_l2_1 = hp.Float('l2_1', min_value=1e-6, max_value=1e-2, sampling='log', default=1e-4)\n","        hp_l2_2 = hp.Float('l2_2', min_value=1e-6, max_value=1e-2, sampling='log', default=1e-4)\n","        hp_l2_3 = hp.Float('l2_3', min_value=1e-6, max_value=1e-2, sampling='log', default=1e-4)\n","\n","        x = Dense(\n","            hp_units_1,\n","            activation='relu',\n","            kernel_regularizer=regularizers.l2(hp_l2_1)  \n","        )(x)\n","\n","\n","        x = Dense(\n","            hp_units_2,\n","            activation='relu',\n","            kernel_regularizer=regularizers.l2(hp_l2_2) \n","        )(x)\n","\n","\n","        x = Dense(\n","            hp_units_3,\n","            activation='relu',\n","            kernel_regularizer=regularizers.l2(hp_l2_3) \n","        )(x)\n","\n","        x = Dropout(hp.Float('dropout_rate', min_value=0.3, max_value=0.7, step=0.1, default=0.5))(x)\n","\n","\n","        predictions = Dense(len(labels), activation='sigmoid')(x)\n","\n","        model = keras.Model(inputs=inputs, outputs=predictions)\n","        model.compile(optimizer=Adam(learning_rate=0.00001),\n","                  loss=\"binary_crossentropy\",\n","                  metrics=['accuracy',tf.keras.metrics.AUC(name=\"auc\",  multi_label=True,num_labels=len(labels)),\n","                             tf.keras.metrics.AUC(name=\"auc_roc\", curve=\"ROC\", multi_label=True,num_labels=len(labels)),\n","                            tf.keras.metrics.AUC(name=\"auc_pr\", curve=\"PR\", multi_label=True,num_labels=len(labels)),\n","                            tf.keras.metrics.Precision(name=\"precision\"),\n","                            tf.keras.metrics.Recall(name=\"recall\"),\n","                            F1Score(num_classes=len(labels),average='weighted',threshold=0.5),\n","                           HammingLoss(),\n","                            tfr.keras.metrics.MeanAveragePrecisionMetric(name=\"map\")])\n","        return model"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2023-10-08T06:04:15.860907Z","iopub.status.busy":"2023-10-08T06:04:15.860551Z","iopub.status.idle":"2023-10-08T06:04:15.877079Z","shell.execute_reply":"2023-10-08T06:04:15.876044Z","shell.execute_reply.started":"2023-10-08T06:04:15.860874Z"},"trusted":true},"outputs":[],"source":["img_shape=(299,299,3)"]},{"cell_type":"markdown","metadata":{},"source":["### Hyperparameter Tuning Configuration\n","\n","In this code the configuration for hyperparameter tuning of a neural network model is defined using the Keras Tuner library.\n","\n","#### Hyperparameter Tuner\n","\n","- The code imports the Keras Tuner library as `kt` for consistency.\n","- A hyperparameter tuner is created using the `RandomSearch` method, which performs a random search over a defined hyperparameter space.\n","- The tuner is set to search for hyperparameters that optimize the \"val_f1_score,\" with the goal of maximizing this metric.\n","\n","#### Objective and Trials\n","\n","- The objective of the tuning is set to maximize the \"val_f1_score,\" indicating that the F1 score on the validation dataset is the primary metric of interest.\n","- The `max_trials` parameter is set to 5, determining the number of trials to run during hyperparameter optimization. Adjust this value based on the computational resources available.\n","\n","#### Tuning Directory\n","\n","- The directory for storing tuning-related information is specified as 'hyperparameter_tuning'.\n","- The project name is designated as 'custom_image_augmentation' for easy reference.\n","\n","This code sets up the configuration for hyperparameter tuning, enabling the search for optimal model hyperparameters while focusing on maximizing the F1 score on the validation dataset.\n"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2023-10-08T06:04:15.879240Z","iopub.status.busy":"2023-10-08T06:04:15.878362Z","iopub.status.idle":"2023-10-08T06:04:22.972980Z","shell.execute_reply":"2023-10-08T06:04:22.971989Z","shell.execute_reply.started":"2023-10-08T06:04:15.879176Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/inception_v3/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5\n","87910968/87910968 [==============================] - 0s 0us/step\n"]}],"source":["import keras_tuner as kt  \n","tuner = RandomSearch(\n","    model_builder,\n","    objective=kt.Objective(\"val_f1_score\", direction=\"max\"), \n","    max_trials=5,  \n","    directory='hyperparameter_tuning',\n","    project_name='custom_image_augmentation'\n",")"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2023-10-08T06:04:22.974830Z","iopub.status.busy":"2023-10-08T06:04:22.974477Z","iopub.status.idle":"2023-10-08T06:04:22.984870Z","shell.execute_reply":"2023-10-08T06:04:22.983905Z","shell.execute_reply.started":"2023-10-08T06:04:22.974800Z"},"trusted":true},"outputs":[{"data":{"text/plain":["<keras_tuner.tuners.randomsearch.RandomSearch at 0x7de235fa6440>"]},"execution_count":14,"metadata":{},"output_type":"execute_result"}],"source":["tuner"]},{"cell_type":"markdown","metadata":{},"source":["### Learning Rate Adjustment Callback\n","\n","In this code  a custom callback class named `LRA` is defined for learning rate adjustment during the training of a deep learning model. The primary purpose of this callback is to dynamically adjust the learning rate based on various conditions, optimizing the training process. Here's an explanation of the code:\n","\n","#### Callback Class\n","\n","- The `LRA` callback class is designed to adjust the learning rate during training. It accepts multiple parameters such as initial learning rate, patience, stop patience, threshold, factor, dwell, batches, initial epoch, and total epochs.\n","\n","#### Learning Rate Adjustment\n","\n","- The callback monitors training and validation metrics, such as accuracy, F1 score, validation loss, and more. Depending on specified criteria, the learning rate is adjusted accordingly.\n","\n","- The callback tracks factors like the improvement of validation accuracy, validation F1 score, and the reduction of validation loss.\n","\n","- If specified thresholds are not met or improvement is lacking, the callback reduces the learning rate and continues training. The process can be customized with dwell functionality to revert to better points in the optimization space.\n","\n","- If adjustments continue without substantial improvement within the defined stop patience, the training process is halted.\n","\n","The `LRA` callback offers dynamic control over the learning rate, ensuring that the deep learning model converges effectively and efficiently during training. It serves as a valuable tool for fine-tuning the model's performance.\n","\n","\n"]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2023-10-08T06:04:22.990719Z","iopub.status.busy":"2023-10-08T06:04:22.989958Z","iopub.status.idle":"2023-10-08T06:04:23.364080Z","shell.execute_reply":"2023-10-08T06:04:23.362873Z","shell.execute_reply.started":"2023-10-08T06:04:22.990682Z"},"trusted":true},"outputs":[],"source":["\n","class LRA(keras.callbacks.Callback):\n","    def __init__(self,initial_lr, patience,stop_patience, threshold, factor, dwell, batches, initial_epoch,epochs):\n","        super(LRA, self).__init__()\n","        self.patience=patience \n","        self.stop_patience=stop_patience \n","        self.threshold=threshold \n","        self.factor=factor \n","        self.dwell=dwell\n","        self.batches=batches \n","        self.initial_epoch=initial_epoch\n","        self.epochs=epochs\n","        self.count=0 \n","        self.stop_count=0   \n","        self.highest_f1_score=0\n","        self.best_epoch=1         \n","        self.initial_lr=initial_lr         \n","        self.highest_tracc=0.0 \n","        self.lowest_vloss=np.inf \n","    def on_epoch_end(self, epoch, logs=None):  \n","        lr=float(tf.keras.backend.get_value(self.model.optimizer.lr)) \n","        current_lr=lr\n","        v_loss=logs.get('val_loss')  \n","        acc=logs.get('accuracy')  \n","        loss=logs.get('loss')\n","        auc=logs.get('auc')\n","        v_auc=logs.get('val_auc')\n","        Map=logs.get('map')\n","        val_map=logs.get('val_map')\n","        v_acc = logs.get('val_accuracy')\n","        val_precision = logs.get('val_precision')\n","        val_recall = logs.get('val_recall')\n","        val_f1_score = logs.get('val_f1_score')\n","        val_hamming_loss = logs.get('val_hamming_loss')\n","        precision = logs.get('precision')\n","        recall = logs.get('recall')\n","        f1_score = logs.get('f1_score')\n","        hamming_loss = logs.get('hamming_loss')\n","        auc_roc=logs.get('auc_roc')\n","        val_auc_roc=logs.get('val_auc_roc')\n","        auc_pr=logs.get('auc_pr')\n","        val_auc_pr=logs.get('val_auc_pr')\n","        if v_acc < self.threshold: \n","            monitor='val_accuracy'\n","            if epoch ==0:\n","                pimprov=0.0\n","            else:\n","                pimprov= (v_acc-self.highest_tracc )*100/self.highest_tracc\n","            if v_acc>self.highest_tracc: \n","                self.highest_tracc=v_acc \n","                self.best_weights=self.model.get_weights() \n","                self.count=0 \n","                self.stop_count=0 \n","                if v_loss<self.lowest_vloss:\n","                    self.lowest_vloss=v_loss\n","                if val_f1_score>self.highest_f1_score :\n","                    self.highest_f1_score = val_f1_score\n","                color= (0,255,0)\n","                self.best_epoch=epoch + 1              \n","            else: \n","                if self.count>=self.patience -1: \n","                    color=(245, 170, 66)\n","                    lr= lr* self.factor \n","                    tf.keras.backend.set_value(self.model.optimizer.lr, lr) \n","                    self.count=0 \n","                    self.stop_count=self.stop_count + 1 \n","                    self.count=0 # reset counter\n","                    if self.dwell:\n","                        self.model.set_weights(self.best_weights)                     \n","                    else:\n","                        if v_loss<self.lowest_vloss:\n","                            self.lowest_vloss=v_loss    \n","                        if val_f1_score>self.highest_f1_score :\n","                            self.highest_f1_score = val_f1_score\n","                else:\n","                    self.count=self.count +1   \n","        elif val_f1_score < 0.8:\n","            monitor='val_f1_score'\n","            if epoch ==0:\n","                pimprov=0.0\n","            else:\n","                pimprov= (val_f1_score-self.highest_f1_score )*100/self.highest_f1_score\n","            if val_f1_score>self.highest_f1_score: \n","                self.highest_f1_score=val_f1_score \n","                self.best_weights=self.model.get_weights() \n","                self.count=0 \n","                self.stop_count=0 \n","                if v_loss<self.lowest_vloss:\n","                    self.lowest_vloss=v_loss\n","                if v_acc>self.highest_tracc:\n","                    self.highest_tracc= v_acc\n","                color= (0,255,0)\n","                self.best_epoch=epoch + 1          \n","            else: \n","                if self.count>=self.patience -1: \n","                    color=(245, 170, 66)\n","                    lr= lr* self.factor \n","                    tf.keras.backend.set_value(self.model.optimizer.lr, lr) \n","                    self.count=0 \n","                    self.stop_count=self.stop_count + 1 \n","                    self.count=0 \n","                    if self.dwell:\n","                        self.model.set_weights(self.best_weights)                        \n","                    else:\n","                        if v_loss<self.lowest_vloss:\n","                            self.lowest_vloss=v_loss  \n","                        if v_acc>self.highest_tracc:\n","                            self.highest_tracc= v_acc\n","                else:\n","                    self.count=self.count +1           \n","        else: \n","            monitor='val_loss'\n","            if epoch ==0:\n","                pimprov=0.0\n","            else:\n","                pimprov= (self.lowest_vloss- v_loss )*100/self.lowest_vloss\n","            if v_loss< self.lowest_vloss: \n","                self.lowest_vloss=v_loss          \n","                self.best_weights=self.model.get_weights() \n","                self.count=0 \n","                self.stop_count=0  \n","                color=(0,255,0)                \n","                self.best_epoch=epoch + 1 \n","            else: \n","                if self.count>=self.patience-1: \n","                    color=(245, 170, 66)\n","                    lr=lr * self.factor                   \n","                    self.stop_count=self.stop_count + 1 \n","                    self.count=0 \n","                    tf.keras.backend.set_value(self.model.optimizer.lr, lr) \n","                    if self.dwell:\n","                        self.model.set_weights(self.best_weights) \n","                else: \n","                    self.count =self.count +1                    \n","                if v_acc>self.highest_tracc:\n","                    self.highest_tracc= v_acc\n","                if val_f1_score>self.highest_f1_score :\n","                    self.highest_f1_score = val_f1_score\n","        \n","        if self.stop_count> self.stop_patience - 1: \n","            msg=f' training has been halted at epoch {epoch + 1} after {self.stop_patience} adjustments of learning rate with no improvement'\n","            print_in_color(msg, (0,255,255), (55,65,80))\n","            self.model.stop_training = True "]},{"cell_type":"code","execution_count":16,"metadata":{"execution":{"iopub.execute_input":"2023-10-08T06:04:23.366240Z","iopub.status.busy":"2023-10-08T06:04:23.365784Z","iopub.status.idle":"2023-10-08T06:04:23.386332Z","shell.execute_reply":"2023-10-08T06:04:23.385015Z","shell.execute_reply.started":"2023-10-08T06:04:23.366181Z"},"trusted":true},"outputs":[{"data":{"text/plain":["235"]},"execution_count":16,"metadata":{},"output_type":"execute_result"}],"source":["train_steps=int(np.ceil(len(train_generator.labels)/batch_size))\n","train_steps"]},{"cell_type":"code","execution_count":17,"metadata":{"execution":{"iopub.execute_input":"2023-10-08T06:04:23.388118Z","iopub.status.busy":"2023-10-08T06:04:23.387756Z","iopub.status.idle":"2023-10-08T06:04:23.395002Z","shell.execute_reply":"2023-10-08T06:04:23.394151Z","shell.execute_reply.started":"2023-10-08T06:04:23.388087Z"},"trusted":true},"outputs":[],"source":["working_dir = '/kaggle/working/'"]},{"cell_type":"markdown","metadata":{},"source":["### Training Configuration Parameters\n","\n","This section outlines key parameters used to configure the training process of a deep learning model:\n","\n","#### `epochs`\n","\n","- `epochs`: The total number of training epochs. In this example, it is set to 20. Each epoch represents one complete iteration through the entire training dataset.\n","\n","#### `patience`\n","\n","- `patience`: This parameter determines the number of epochs to wait before considering adjustments to the learning rate if the monitored value (e.g., accuracy) does not improve. Here, it is set to 8.\n","\n","#### `stop_patience`\n","\n","- `stop_patience`: Specifies the number of epochs to wait before halting the training if the monitored value does not exhibit improvement. A value of 2 means that if the model's performance does not improve over the course of 2 consecutive epochs, the training process will be stopped.\n","\n","#### `threshold`\n","\n","- `threshold`: The threshold is a critical value that impacts which aspect of the model's performance is monitored. If the training accuracy falls below this threshold (0.65 in this example), the callback focuses on monitoring the training accuracy. If the training accuracy is equal to or exceeds this threshold, the callback switches to monitoring the validation loss.\n","\n","#### `factor`\n","\n","- `factor`: Represents the rate at which the learning rate is reduced when an adjustment is triggered. A factor of 0.1 signifies that the learning rate will be reduced to one-tenth of its previous value.\n","\n","#### `dwell`\n","\n","- `dwell`: When set to `True`, this parameter enables an experimental feature. If the monitored metric does not improve in the current epoch, the model's weights will be reset to those of the previous epoch, aiming to explore better training outcomes.\n","\n","#### `ask_epoch`\n","\n","- `ask_epoch`: Specifies the number of epochs to run before prompting the user or training process to inquire about halting training. It is set to 100 in this example.\n","\n","#### `batches`\n","\n","- `batches`: Defines the number of training batches to process per epoch. The specific value may depend on your dataset and hardware setup.\n","\n","#### `csv_path`\n","\n","- `csv_path`: Represents the path where the training-related CSV file will be saved. It's defined by joining the `working_dir` and 'my_csv'.\n","\n","These training configuration parameters are essential for controlling the training dynamics, ensuring efficient learning, and monitoring the model's performance.\n"]},{"cell_type":"code","execution_count":18,"metadata":{"execution":{"iopub.execute_input":"2023-10-08T06:04:23.396903Z","iopub.status.busy":"2023-10-08T06:04:23.396550Z","iopub.status.idle":"2023-10-08T06:04:23.405091Z","shell.execute_reply":"2023-10-08T06:04:23.404181Z","shell.execute_reply.started":"2023-10-08T06:04:23.396869Z"},"trusted":true},"outputs":[],"source":["epochs =20\n","patience= 8 \n","stop_patience =2 \n","threshold=.65 \n","factor=0.1 \n","dwell=True \n","ask_epoch=100 \n","batches=train_steps\n","csv_path=os.path.join(working_dir,'my_csv')\n"]},{"cell_type":"markdown","metadata":{},"source":["### Custom Colorful Text Printer\n","\n","In this code a custom text printer function called `print_in_color` is defined. This function enables the printing of text messages in a user-specified foreground color on a custom background color. Here's an explanation of the code:\n","\n","#### Custom Text Printer Function\n","\n","- The `print_in_color` function is designed to display text in a customized color scheme.\n","\n","- It accepts three parameters:\n","\n","  - `txt_msg`: The text message to be printed.\n","  - `fore_tupple`: A tuple representing the RGB values of the foreground color.\n","  - `back_tupple`: A tuple representing the RGB values of the background color.\n","\n","- Within the function, an ANSI escape code is constructed to specify the desired text and background colors using the RGB values provided in the tuples.\n","\n","- The `print` function is used to display the `txt_msg` with the specified colors.\n","\n","- After printing, a reset code (`'\\33[0m'`) is included to return the print color to the default black.\n","\n","This `print_in_color` function serves as a helpful tool for enhancing the visual appeal of text in code output or command line interfaces by incorporating custom colors.\n","\n"]},{"cell_type":"code","execution_count":19,"metadata":{"execution":{"iopub.execute_input":"2023-10-08T06:04:23.407351Z","iopub.status.busy":"2023-10-08T06:04:23.406629Z","iopub.status.idle":"2023-10-08T06:04:23.417234Z","shell.execute_reply":"2023-10-08T06:04:23.416308Z","shell.execute_reply.started":"2023-10-08T06:04:23.407302Z"},"trusted":true},"outputs":[],"source":["import time\n","import datetime\n","from datetime import datetime\n","def print_in_color(txt_msg,fore_tupple,back_tupple,):\n","\n","    rf,gf,bf=fore_tupple\n","    rb,gb,bb=back_tupple\n","    msg='{0}' + txt_msg\n","    mat='\\33[38;2;' + str(rf) +';' + str(gf) + ';' + str(bf) + ';48;2;' + str(rb) + ';' +str(gb) + ';' + str(bb) +'m' \n","    print(msg .format(mat), flush=True)\n","    print('\\33[0m', flush=True) \n","    return"]},{"cell_type":"markdown","metadata":{},"source":["### Custom Callback for Clearing Training Output\n","\n","In this code  a custom callback class named `ClearTrainingOutput` is defined. This callback is intended to clear the training output from the console window once the training is complete. Here's an explanation of the code:\n","\n","#### Custom Callback Class\n","\n","- The `ClearTrainingOutput` class extends the functionality of the `tf.keras.callbacks.Callback` class to define custom behavior when the training ends.\n","\n","- Within the `on_train_end` method, the code checks the operating system (OS) using `os.name`. If the OS is Windows (indicated by `'nt'`), the `cls` command is executed to clear the console. For non-Windows OS (e.g., Unix-like systems), the `clear` command is used for the same purpose.\n","\n","- This clearing action ensures that the console window is free from the training output, providing a cleaner and more organized environment.\n","\n","After defining this custom callback, it is used when conducting a hyperparameter search using the Keras Tuner.\n","\n"]},{"cell_type":"code","execution_count":20,"metadata":{"execution":{"iopub.execute_input":"2023-10-08T06:04:23.419452Z","iopub.status.busy":"2023-10-08T06:04:23.418620Z","iopub.status.idle":"2023-10-08T11:11:16.500501Z","shell.execute_reply":"2023-10-08T11:11:16.499641Z","shell.execute_reply.started":"2023-10-08T06:04:23.419419Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Trial 5 Complete [01h 01m 28s]\n","val_f1_score: 0.15849845111370087\n","\n","Best val_f1_score So Far: 0.2259935587644577\n","Total elapsed time: 05h 06m 53s\n"]}],"source":["\n","import os\n","class ClearTrainingOutput(tf.keras.callbacks.Callback):\n","    def on_train_end(self, logs=None):\n","        os.system('cls' if os.name == 'nt' else 'clear')\n","tuner.search(\n","    train_generator,\n","    validation_data=val_generator,\n","    callbacks=[LRA(initial_lr=0.00001,patience=patience,stop_patience=stop_patience, threshold=threshold,\n","                   factor=factor,dwell=dwell, batches=batches,initial_epoch=0,epochs=epochs), ClearTrainingOutput()], epochs=epochs,\n","    validation_steps=None, shuffle=False, initial_epoch=0\n",")\n","\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["### Display Best Hyperparameters\n","\n","In this code the best hyperparameters obtained from the hyperparameter tuning process are retrieved and displayed. Here's an explanation of the code:\n","\n","#### Retrieving Best Hyperparameters\n","\n","- The `tuner.get_best_hyperparameters(num_trials=1)` method is used to obtain the best hyperparameters. With `num_trials=1`, we retrieve the top-performing hyperparameters.\n","\n","#### Displaying Best Hyperparameters\n","\n","- The code prints the best hyperparameters, including values for `Dense Units 1`, `Dense Units 2`, `Dense Units 3`, `L2 Regularization 1`, `L2 Regularization 2`, `L2 Regularization 3`, and `Dropout Rate`.\n","\n","- These hyperparameters are crucial for configuring the model architecture and training settings. Displaying them allows us to understand the optimal settings found during hyperparameter tuning.\n","\n","By showcasing the best hyperparameters, they can be used to build and train the final deep learning model with the most effective configuration.\n"]},{"cell_type":"code","execution_count":21,"metadata":{"execution":{"iopub.execute_input":"2023-10-08T11:11:16.506993Z","iopub.status.busy":"2023-10-08T11:11:16.504485Z","iopub.status.idle":"2023-10-08T11:11:16.517251Z","shell.execute_reply":"2023-10-08T11:11:16.516344Z","shell.execute_reply.started":"2023-10-08T11:11:16.506954Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Best Hyperparameters:\n","Dense Units 1: 512\n","Dense Units 2: 896\n","Dense Units 3: 384\n","L2 Regularization 1: 2.5803151973980912e-05\n","L2 Regularization 2: 4.289969209505223e-06\n","L2 Regularization 3: 0.0017392802275116102\n","Dropout Rate: 0.3\n"]}],"source":["\n","best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n","\n","print(\"Best Hyperparameters:\")\n","print(f\"Dense Units 1: {best_hps.get('dense_units_1')}\")\n","print(f\"Dense Units 2: {best_hps.get('dense_units_2')}\")\n","print(f\"Dense Units 3: {best_hps.get('dense_units_3')}\")\n","print(f\"L2 Regularization 1: {best_hps.get('l2_1')}\")\n","print(f\"L2 Regularization 2: {best_hps.get('l2_2')}\")\n","print(f\"L2 Regularization 3: {best_hps.get('l2_3')}\")\n","print(f\"Dropout Rate: {best_hps.get('dropout_rate')}\")"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":4}
