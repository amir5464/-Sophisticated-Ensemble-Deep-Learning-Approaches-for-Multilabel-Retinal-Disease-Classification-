{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2023-10-23T10:59:10.123865Z","iopub.status.busy":"2023-10-23T10:59:10.123464Z","iopub.status.idle":"2023-10-23T10:59:19.938021Z","shell.execute_reply":"2023-10-23T10:59:19.936917Z","shell.execute_reply.started":"2023-10-23T10:59:10.123829Z"},"trusted":true},"outputs":[],"source":["import os\n","import numpy as np\n","from PIL import Image\n","import tensorflow as tf \n","import pandas as pd"]},{"cell_type":"markdown","metadata":{},"source":["### Data Splitting for Disease Classification\n","\n","This code  demonstrates how to split a dataset for disease classification tasks, ensuring that each class is represented in training, validation, and test sets. The dataset is filtered to include only instances where `Disease_Risk=1` and then divided into three subsets: training, validation, and test sets.\n","\n","#### Data Filtering\n","\n","- The code starts by reading the dataset from a CSV file and filters it to include only rows where `Disease_Risk=1`.\n","\n","- It calculates the size of the filtered data (`filtered_size`) and extracts the label columns.\n","\n","- The positive instances for each label are counted and sorted.\n","\n","#### Data Splitting\n","\n","- The data is iteratively split for each label, ensuring that the distribution of positive instances is preserved in each split. For each label:\n","  - It splits the data into a training part, a validation part, and a test part using `train_test_split` from scikit-learn.\n","  - The split indices are stored in separate dictionaries for each label: `train_splits`, `val_splits`, and `test_splits`.\n","  - The samples used for the splits are removed from the remaining data.\n","\n","- After iterating through all labels, the code combines the splits to obtain the final training, validation, and test sets by selecting the samples based on the stored indices.\n","\n","#### Summary\n","\n","- The code provides a systematic approach to ensure that positive instances of each class are represented in training, validation, and test sets, making it suitable for disease classification tasks.\n","\n","- It calculates and prints the sizes of the resulting sets: `train_data`, `val_data`, and `test_data`.\n","\n","This data splitting approach helps maintain class balance in each subset, which is crucial for training machine learning models on imbalanced datasets.\n","\n","\n"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2023-10-23T10:59:19.941170Z","iopub.status.busy":"2023-10-23T10:59:19.940270Z","iopub.status.idle":"2023-10-23T10:59:20.679661Z","shell.execute_reply":"2023-10-23T10:59:20.678554Z","shell.execute_reply.started":"2023-10-23T10:59:19.941122Z"},"trusted":true},"outputs":[{"data":{"text/plain":["(7502, 934, 953)"]},"execution_count":2,"metadata":{},"output_type":"execute_result"}],"source":["data = pd.read_csv('/kaggle/input/updated-csv/data_file.csv')\n","filtered_data = data[data['Disease_Risk'] == 1]\n","\n","filtered_size = len(filtered_data)\n","label_columns = filtered_data.columns[2:-1]\n","\n","label_counts = filtered_data[label_columns].sum().sort_values()\n","\n","from sklearn.model_selection import train_test_split\n","from collections import defaultdict\n","\n","\n","train_splits = defaultdict(list)\n","val_splits = defaultdict(list)\n","test_splits = defaultdict(list)\n","\n","remaining_data = filtered_data.copy()\n","\n","for label in label_counts.index:\n","\n","    train_part, temp_part = train_test_split(remaining_data[remaining_data[label] == 1], test_size=0.2, random_state=42, stratify=remaining_data[remaining_data[label] == 1][label])\n","    val_part, test_part = train_test_split(temp_part, test_size=0.5, random_state=42, stratify=temp_part[label])\n","    \n","\n","    train_splits[label].extend(train_part.index.tolist())\n","    val_splits[label].extend(val_part.index.tolist())\n","    test_splits[label].extend(test_part.index.tolist())\n","\n","    remaining_data = remaining_data.drop(train_part.index)\n","    remaining_data = remaining_data.drop(val_part.index)\n","    remaining_data = remaining_data.drop(test_part.index)\n","\n","\n","train_idx = [idx for idx_list in train_splits.values() for idx in idx_list]\n","val_idx = [idx for idx_list in val_splits.values() for idx in idx_list]\n","test_idx = [idx for idx_list in test_splits.values() for idx in idx_list]\n","\n","train_data = filtered_data.loc[train_idx]\n","val_data = filtered_data.loc[val_idx]\n","test_data = filtered_data.loc[test_idx]\n","\n","len(train_data), len(val_data), len(test_data)"]},{"cell_type":"markdown","metadata":{},"source":["### Data Augmentation and Combining with Normal Data\n","\n","In this code  the dataset is further processed by augmenting the data for the positive class (diseased) and combining it with normal data. This is done to ensure a balanced dataset for training and evaluation.\n","\n","#### Augmenting Diseased Data\n","\n","- Initially, the code selects the rows from the dataset where `Disease_Risk=0` (normal instances) and stores them in the `normal` variable.\n","\n","- The normal data is then split into a training part and a temporary part, followed by splitting the temporary part into validation and test parts using `train_test_split`. The split indices are stored in the corresponding variables (`normaltrain_data`, `normalval_data`, `normaltest_data`).\n","\n","#### Combining with Diseased Data\n","\n","- The code concatenates the normal data splits with the corresponding splits for the diseased data, which were previously obtained. This results in the combination of both normal and diseased data for the training, validation, and test sets (`train_data`, `val_data`, `test_data`).\n","\n","- The code calculates and prints the sizes of the resulting sets to confirm the balanced distribution of samples.\n","\n","This approach is particularly useful for ensuring that the model has sufficient exposure to both normal and diseased samples during training, promoting robust and balanced model performance.\n","\n","\n"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2023-10-23T10:59:20.681415Z","iopub.status.busy":"2023-10-23T10:59:20.680856Z","iopub.status.idle":"2023-10-23T10:59:20.686779Z","shell.execute_reply":"2023-10-23T10:59:20.685597Z","shell.execute_reply.started":"2023-10-23T10:59:20.681385Z"},"trusted":true},"outputs":[],"source":["normal = data[data['Disease_Risk'] == 0]\n","normaltrain_data, temp_data = train_test_split(normal, test_size=0.4, random_state=42)\n","normalval_data, normaltest_data = train_test_split(temp_data, test_size=0.5, random_state=42)\n","train_data = pd.concat([train_data,normaltrain_data ])\n","val_data = pd.concat([val_data,normalval_data ])\n","test_data = pd.concat([test_data,normaltest_data ])"]},{"cell_type":"markdown","metadata":{},"source":["### Organizing Data into Split Directories\n","\n","In this code  the dataset is organized into split directories, including training, validation, and test sets. This is a common step when preparing data for machine learning experiments. Here's what the code does:\n","\n","#### Destination Directory\n","\n","- The code defines the `base_dest_dir`, which is the destination directory where the split images will be copied. You can specify your desired destination directory.\n","\n","#### Split Folders\n","\n","- The code specifies the split folders, which include 'train', 'val', and 'test'. These folders will contain the respective data splits.\n","\n","#### Iterating Over Splits\n","\n","- The code iterates over each split, creating a destination directory for each split (e.g., `/kaggle/working/data/train`, `/kaggle/working/data/val`, `/kaggle/working/data/test`).\n","\n","- For each split, it iterates over the rows in the corresponding data (e.g., `train_data`, `val_data`, `test_data`), where each row represents an image and its associated information.\n","\n","- It extracts the image path from the dataset (you may need to replace `'IMG_DIR'` with the actual column name) and obtains the image file name using `os.path.basename(image_path)`.\n","\n","- It constructs the destination path for the image in the split directory.\n","\n","- The image file is copied from its original location to the destination directory using `shutil.copy`.\n","\n","This code structure is commonly used to prepare data for machine learning experiments, ensuring that the data is organized into distinct training, validation, and test sets within separate directories.\n","\n"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2023-10-23T10:59:20.739520Z","iopub.status.busy":"2023-10-23T10:59:20.739108Z","iopub.status.idle":"2023-10-23T11:01:01.708270Z","shell.execute_reply":"2023-10-23T11:01:01.707056Z","shell.execute_reply.started":"2023-10-23T10:59:20.739486Z"},"trusted":true},"outputs":[],"source":["import os\n","import shutil\n","\n","\n","base_dest_dir = '/kaggle/working/data'\n","\n","\n","split_folders = ['train', 'val', 'test']\n","\n","\n","for split, data in zip(split_folders, [train_data, val_data, test_data]):\n","    split_dest_dir = os.path.join(base_dest_dir, split)\n","    os.makedirs(split_dest_dir, exist_ok=True)  \n","    \n","\n","    for index, row in data.iterrows():\n","        image_path = row['IMG_DIR']  \n","        image_name = os.path.basename(image_path)  \n","        \n","        dest_path = os.path.join(split_dest_dir, image_name)\n","        \n","     \n","        shutil.copy(image_path, dest_path)"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2023-10-23T11:01:01.709955Z","iopub.status.busy":"2023-10-23T11:01:01.709557Z","iopub.status.idle":"2023-10-23T11:01:01.776048Z","shell.execute_reply":"2023-10-23T11:01:01.775016Z","shell.execute_reply.started":"2023-10-23T11:01:01.709928Z"},"trusted":true},"outputs":[],"source":["train_data.to_csv('/kaggle/working/train_data.csv', index=False)"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2023-10-23T11:01:01.779744Z","iopub.status.busy":"2023-10-23T11:01:01.779281Z","iopub.status.idle":"2023-10-23T11:01:01.796346Z","shell.execute_reply":"2023-10-23T11:01:01.795011Z","shell.execute_reply.started":"2023-10-23T11:01:01.779705Z"},"trusted":true},"outputs":[],"source":["val_data.to_csv('/kaggle/working/val_data.csv', index=False)"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2023-10-23T11:01:01.799181Z","iopub.status.busy":"2023-10-23T11:01:01.797873Z","iopub.status.idle":"2023-10-23T11:01:01.812623Z","shell.execute_reply":"2023-10-23T11:01:01.811648Z","shell.execute_reply.started":"2023-10-23T11:01:01.799142Z"},"trusted":true},"outputs":[],"source":["test_data.to_csv('/kaggle/working/test_data.csv', index=False)"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2023-10-23T11:01:41.229860Z","iopub.status.busy":"2023-10-23T11:01:41.229406Z","iopub.status.idle":"2023-10-23T11:02:47.783553Z","shell.execute_reply":"2023-10-23T11:02:47.782412Z","shell.execute_reply.started":"2023-10-23T11:01:41.229827Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["\"/kaggle/working/data\" has been zipped to \"data\"\n"]}],"source":["import shutil\n","\n","\n","directory_to_zip = '/kaggle/working/data'\n","\n","\n","zip_file_name = 'data'\n","\n","\n","shutil.make_archive(zip_file_name, 'zip', directory_to_zip)\n","\n","print(f'\"{directory_to_zip}\" has been zipped to \"{zip_file_name}\"')\n"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":4}
